大家好，我是拭心。

上一讲我们做出了一个很炫的[多模态聊天软件：能看图、能聊天、能生成视频，多模态能力拉满](https://xiaobot.net/post/5f2e43fe-04ef-4694-bda6-4c906026b97c)。但是，还遗留了一个重要的功能没处理：多轮对话。

我们做的多模态聊天软件，少几轮对话都还 OK，但一旦谈话多了，或者一次对话多次生成图片、视频，再去聊天，就会发现，**卡住不动了**。然后你刷新了一下页面，继续聊，会发现**AI 不记得前面聊过什么了**。

每次对话都是从零开始，就像传说中的金鱼——只有 7 秒记忆。这在用户体验上是灾难性的，工程上也是不合格的，商业应用必须要解决这个问题。

这就是我们这篇文章要学习的：**通过上下文工程，让你的 AI 应用怎么聊都有记忆。**

## 一、什么是上下文工程？

上下文，就是我们在使用大模型时，提供给他的输入参数。

在我们的上一讲聊天软件里，每次我们发出消息，接口请求参数里会有个 messages 参数，这就是这次请求的上下文：

![](https://static.xiaobot.net/file/2026-01-13/21332/cf8e712607e239b333fd9158dcc3c113.png!post)

之前的逻辑是，**下次请求会把之前的所有 System 提示词（内置提示词）、User 提示词（用户输入）和接口响应（比如图片数据）都带上**。

这种粗暴的方式会导致 messages 的数据量很快暴涨，尤其是生成图片和视频以后，每次请求都带着几 MB 的图片原始数据，响应速度和成本上必定会很糟糕。

![](https://static.xiaobot.net/file/2026-01-13/21332/8c00c92c3c75ecf051fce2d725bf297f.png!post)

而上下文工程，不是简单的字符串缩减，是对信息流的“精算”。它要解决的核心问题是：

-   如何在有限的窗口内，保留最关键的信息？（短期记忆管理）
    
-   如何让 AI 记住用户的长期偏好？（长期记忆管理）
    
-   如何在成本、性能和体验之间找到平衡点？（工程权衡）
    

接下来，我们先从理论开始。

## 二、像人脑一样设计记忆系统

1968 年，心理学家 Richard Atkinson 和 Richard Shiffrin 提出了著名的“多存储记忆模型”（Multi-Store Model），它将人类记忆系统分为三个独立的层次。

这个模型不仅解释了人类如何记忆，也为我们设计 AI 记忆系统提供了完美的架构蓝图。

![](https://static.xiaobot.net/file/2026-01-13/21332/72617d5daab9095d25369a8ec216af09.png!post)

我们来看看这三层记忆架构。

## 2.1 第一层：感知记忆

首先是感知记忆层。这一层的意思是，人脑就像一个容量有限的行车记录仪，不断接收来自眼睛、耳朵等感官的原始信息，但这些原始信息只能记住 0.5-3 秒，再早的就会被覆盖、遗忘。

对我们的启发是，在 AI 应用中如何减少上下文覆盖？

一个比较容易想到的策略是：**对上下文做清理 ，去掉低质量的。**

用户的原始输入可能会有很多无效信息，比如：

-   文字输入的废话：“请帮我...”
    
-   语音输入的重复表达：“我想要，嗯，就是那种，嗯，赛博朋克风格的，嗯...”
    

通过一些简单的清洗，就可以让上下文里的信息量显著增加，比如：

-   删除口语化表达
    
-   标准化输入格式
    

## 2.2 第二层：短时记忆

在感知记忆那一层的数据，有少量投入注意力的会进入到记忆的第二阶段：短时记忆。

短时记忆是指**在刺激消失后，可保持十几秒到一分钟左右的记忆**。例如，打电话前从电话簿查到一个号码，我们能立即凭记忆复述出来，但通话结束后往往就记不清完整号码——这便是短时记忆的典型表现。

著名的“7±2”法则说的就是这个概念：我们人类大脑一次只能记住 5-9 个信息块，并且能记住的时间约 15-30 秒，需要主动复述才能保持。所以很多产品设计，都将选项限制在 4～5 个，这样最符合用户的认知负荷上线。

**短期记忆对我们的启发是，把 AI 应用的上下文也控制在一定范围内，只保留最核心的几轮对话，让大模型能抓住重点。**

在处理之前，请求 API 时的 messages 列表会无限增加，这会直接影响成本和延迟。我们可以**采用「滑动窗口 + 摘要」的方式优化：**

-   只保留最近 N 轮对话
    
-   超过阈值时，将旧对话压缩为摘要
    

## 2.3 第三层：长期记忆

**长期记忆是指能保持一分钟以上，甚至终身不忘的记忆**。这部分的容量几乎是无限的，就像一个大容量的硬盘，存储着我们所有的知识、经验和技能 。与由表现特征构成的短期记忆不同，长期记忆主要以意义逻辑为载体，因此理解得越深，记得越牢 。

从短期记忆到长期记忆的转化，关键过程是“记忆巩固”。在这个过程中，有意义的编码（例如谐音梗---“饿的话，每日熬一鹰”来记忆八国联军）和策略性的复习（比如艾宾浩斯遗忘曲线）至关重要，能有效对抗遗忘 。

**长期记忆对我们的启发是：AI 应用不仅要管理好当前的“工作上下文”，更要思考如何为用户构建一个持续生长、有意义的“个人知识库”。**

我们开发 AI 应用时，需要理解用户意图的深层“意义”，并将碎片化的交互整合成有价值的、可随时提取的长期经验，而不仅仅是处理孤立的单次对话。

具体来说，我们需要持久化存储的用户数据，比如：

-   用户画像：名字、职业、偏好
    
-   历史摘要：过去对话的关键信息
    

如何做到呢？目前比较大众的策略是**实体提取 + 检索**：

-   自动从对话中提取实体（人名、偏好、事实）
    
-   存入结构化数据库（SQL 或 NoSQL）
    
-   下次对话时，根据语义相似度检索相关信息注入上下文
    

## 2.4 由人脑到 AI 应用

理解了人脑的记忆模型后，我们就可以像人脑一样分层设计 AI 应用的上下文系统：

1\. 原始输入

-   用户的原始 Query 优化，去除废话（如“那个...嗯...帮我查下”）。
    
-   我们需要做的：清洗、去除噪音。
    

2\. 短期聊天记录

-   API 请求参数里的 messages 列表，避免无脑 add。
    
-   我们需要做的：保存在内存中，通过滑动窗口 + 压缩 。
    

3\. 长期的用户特征

-   聊天过程提取用户的画像、偏好、历史事实，持久化到数据库等硬盘。
    
-   我们需要做的：实体提取 + 按需检索。
    

接下来，我们看看业界顶尖团队是怎么做的。

## 三、顶尖团队是怎么做上下文工程的？

通过 OpenAI、Anthropic、Google 等团队的公开分享工程实践，我们可以总结出**上下文工程三大核心策略：转移（Offload）、压缩（Reduce）、缓存（Cache）。**

## 3.1 策略一：转移（Offload）

**转移 是指将非必要的数据转移到外部系统，让上下文窗口更轻**。它的核心思想是：别把所有信息都塞进上下文。

具体来说，让上下文中只保留“指针”而不是“数据本身”，就像操作系统的虚拟内存机制——需要时再加载，不需要时不占用宝贵的工作内存。

假设你的 AI Agent 调用了一个天气 API，返回了一个 5000 行的 JSON：

```javascript
{
"city": "Beijing",
"temperature": 25,
"humidity": 60,
"forecast": [
// ... 4998 行详细数据
]
}
```

错误做法是把整个 JSON 塞进上下文，这会直接导致 Token 爆炸、成本飙升，大部分信息 AI 根本用不上。

使用「转移」策略的做法是这样：上下文里只传递摘要或 Result ID：

```javascript
{
"summary": "北京今天 25°C，晴天，适合出行",
"result_id": "weather_20260112_beijing"
}
```

如果 AI 需要更多细节，再通过 \`result\_id\` 去外部存储查询。

### 实际案例

最近 Manus（一个企业级 Agent 框架） 很火，它的 Agent 的 Scratchpad 机制就是转移策略的一个实际案例。

Manus 在处理复杂任务时，会将中间结果写入“草稿本”（Scratchpad），而不是全部保留在 Context 中，比如：

1\. Agent 执行任务，生成中间结果

2\. 将结果存入数据库，返回一个 ID

3\. Context 中只保留：“已完成数据清洗，结果存储于 \`task\_abc123\`”

4\. 下一步需要时，再加载对应数据

这样做的好处：

-   Token 节省：Context 只有几十个字符，而不是几千行数据
    
-   灵活性：可以随时回溯历史任务
    
-   可维护性：中间结果可以独立调试
    

> 参考阅读：[https://blog.langchain.com/context-engineering-for-agents/](https://blog.langchain.com/context-engineering-for-agents/)

## 3.2 策略二：压缩（Reduce）

上下文压缩的意思很好理解：**当上下文必须保留时，主动缩减其大小，同时保留关键信息**。让信息更密集，比如用 10% 的 Token 保留 90% 的信息。

**上下文压缩有两种方式：基于 LLM 的语义压缩和基于算法的 Token 删除**。

### 3.2.1 基于 LLM 的摘要压缩

当对话历史超过阈值（比如 10 轮），调用一个成本很低的模型（如 deepseek），把最早的几条"压缩"成一段摘要，把摘要 + 最近的消息 = 新的上下文：

![](https://static.xiaobot.net/file/2026-01-13/21332/dc719081e85de61ec2fa2c7585079f3f.png!post)

比如原始对话（约 57 Tokens）：

```none
用户：我想做一个电商网站
AI：好的，你需要什么功能？
用户：要有购物车、支付、订单管理
AI：明白了，你倾向用什么技术栈？
用户：React + Node.js
```

压缩后的摘要（约 26 Tokens）：

```none
用户需求：开发电商网站，需要购物车、支付、订单管理，React + Node.js。
```

这样，上下文里只保留摘要 + 最近的完整对话，既保持了历史连贯性，又控制了成本。

### 3.2.2 基于算法的 Token 删除

微软研究院开发的 [LLMLingua](https://github.com/microsoft/LLMLingua) 是一个基于困惑度（Perplexity）的 Prompt 压缩工具。

它的核心原理是：

1\. 使用一个小型语言模型（如 GPT-2-small）计算每个 Token 的重要性

2\. 删除停用词和冗余表达

3\. 保留关键语义

这里有两个概念需要解释下：**困惑度**和**小语言模型**。

困惑度可以理解为模型对下一个词的"惊讶程度"。举个例子：

> "今天天气很\_"

当你看到"很"这个字，你会自然地想到下一个词可能是"好"、"热"、"冷"等。如果真的出现了"好"，模型一点都不惊讶（低困惑度）；但如果出现了"椅子"，模型会很惊讶（高困惑度），因为"今天天气很椅子"不合理。

这个工具处理 token 的大概思路是：

-   困惑度低的词 = 容易预测 = 可能不太重要（冗余的）
    
-   困惑度高的词 = 难以预测 = 可能是关键信息
    

而小语言模型，是指参数量没那么大，"回答问题"的能力不如大模型，但用来判断词的重要性完全够用。

根据官方介绍，压缩效果还是不错的：

-   压缩比：最高可达 20倍，同时保持原始性能
    
-   延迟降低：20-30%
    
-   成本节省：Token 数量直接减少，API 费用大幅下降
    

举个例子：

> "请你帮我详细地分析一下这个非常复杂的问题，我希望你能够给出一个完整的答案。"

压缩后可能变成：

> "请分析这个复杂问题，给出完整答案。

语义几乎不变，但 Token 减少了 60%。

它的压缩流程：

> 原始 Prompt → 小模型逐词计算困惑度 → 删掉低困惑度（冗余）的词 → 压缩后的 Prompt

这里多提一嘴，进阶版的 LLMLingua-2 已集成到 LangChain 和 LlamaIndex 框架中，说明这种思路效果真的不错。

> 延伸阅读：
> 
> 项目主页：[https://llmlingua.com/](https://llmlingua.com/)
> 
> 微软研究院相关文章：[https://www.microsoft.com/en-us/research/blog/llmlingua-innovating-llm-efficiency-with-prompt-compression/](https://www.microsoft.com/en-us/research/blog/llmlingua-innovating-llm-efficiency-with-prompt-compression)

## 3.3 策略三：缓存（Cache）

上下文缓存的思路很简单：**让"不变的部分"只算一次**。就像浏览器缓存 JS 文件，首次加载慢，后续秒开。

想想看，AI 应用的 System Prompt 可能有 10 万字的产品手册，如果每次用户问问题，都要把这 10 万字重新传一遍、算一遍，这不是很浪费吗？

好在，现在这种问题已经被解决了。

当你发送一个 Prompt 给大模型时，服务器需要做两件事：

```none
输入文本 → 计算每个 Token 的向量表示（很耗时）→ 理解上下文 → 生成回答
```

中间的"计算向量表示"这一步非常耗时，而且按 Token 数量收费。

上下文缓存就是把"算过的东西"记住，下次直接使用之前的计算结果。这样可以跳过"计算"这一步，提升响应速度和节省成本。

当我们去看模型价格表时，一般都会有一个「缓存命中价格」，并且这个价格便宜很多。这里的缓存命中就是说输入的内容相同时。

![](https://static.xiaobot.net/file/2026-01-13/21332/693329c4973c8eda169d183863a33acb.png!post)

### 3.3.1 各家模型的缓存机制对比

主流的大模型都支持缓存，但有的自动开启、有的需要手动开启：

![](https://static.xiaobot.net/file/2026-01-13/21332/e436d680896044f62cc61cb85c28cf0d.png!post)

自动开启缓存的模型，我们只需要正常调用 API 即可，但要保证**缓存内容必须在 Prompt 的开头，目前的缓存策略对顺序敏感**。返回值中会有 \`prompt\_cache\_hit\_tokens\` 字段告诉你命中了多少。

少数模型比如 Claude 需要手动标记，标记方式是在 system 中加 \`cache\_control\` 参数：

```python
system=[{
    "type": "text",
    "text": "这里是产品手册...(10万Token)...",
    "cache_control": {"type": "ephemeral"}  # 标记为可缓存
}]
```

手动添加标记后，首次请求后会写入缓存，后续 5 分钟内的请求都走缓存（成本大降）。

> 延伸阅读：
> 
> [https://platform.openai.com/docs/guides/prompt-caching](https://platform.openai.com/docs/guides/prompt-caching)

## 3.4 三大策略的组合拳

在实际项目中，前面的三个上下文策略经常组合使用。

比如你要开发一个企业级客服机器人，可以这样应用策略：

1\. 缓存：将 5 万字的产品手册作为 System Prompt 缓存起来

2\. 转移：历史工单数据存入数据库，只在需要时检索

3\. 压缩：当对话超过 20 轮时，自动调用 deepseek 生成摘要

接下来，我们进入实战环节，通过 2 个案例熟悉一下上下文工程的代码实现。

## 四、实战一：给聊天加上短期记忆能力

我们给上一讲开发的多模态聊天应用，增加一个短期的记忆管理功能。**目标是：无论对话进行多久，上下文始终不会太长，保持在可控范围内。**

这个短期记忆管理，使用「基于 LLM 的摘要压缩」策略。核心思路：

1\. 设定一个窗口大小（比如 10 条消息）

2\. 当对话超过 10 条时，把最早的几条"压缩"成一段摘要

3\. 摘要 + 最近的消息 = 新的上下文

4\. 下次再超出时，把新的旧消息和之前的摘要一起再压缩

## 4.1 核心参数

我们的实现中有两个关键参数：

```typescript
const MAX_MESSAGES = 10;        // 滑动窗口大小
const SUMMARY_THRESHOLD = 4;    // 每次压缩的消息数
```

参数含义：

![](https://static.xiaobot.net/file/2026-01-13/21332/ce421f70376ff933042441333876cdd7.png!post)

为什么这么设置？

-   10 条消息大约是 5 轮对话，足够保持上下文连贯性
    
-   每次压缩 4 条（2 轮），不会一次性丢失太多信息
    
-   保留最近 6 条，确保最新的对话完整保留
    

## 4.2 实现流程

![](https://static.xiaobot.net/file/2026-01-13/21332/22967e494d2bea4bdb1d023f45866d55.png!post)

**压缩流程如上图所示，当消息数超出 10 条时，切分出要压缩的消息，然后调用 AI 生成摘要。**

假设用户和 AI 进行了 12 轮对话（24 条消息），现在要发送第 13 条消息。

**原始 12 条消息：**

```none
消息1 (user):      "你好，我叫张三"
消息2 (assistant): "你好张三！有什么可以帮你的？"
消息3 (user):      "帮我生成一张猫的图片"
消息4 (assistant): "好的，已为你生成猫咪图片 [图片]"
消息5 (user):      "再画一只狗"
消息6 (assistant): "好的，这是一只可爱的狗狗 [图片]"
────────────────── ↑ 以上 6 条会被压缩 ↑ ──────────────────
消息7 (user):      "这只狗太可爱了"
消息8 (assistant): "谢谢夸奖！需要我调整什么吗？"
消息9 (user):      "把背景改成蓝天"
消息10(assistant): "好的，背景已改为蓝天 [图片]"
消息11(user):      "完美！"
消息12(assistant): "很高兴你喜欢！"
────────────────── ↑ 以上 6 条原文保留 ↑ ──────────────────
```

**压缩后的结果：**

```none
System Prompt: 
你是一个多模态 AI 助手...
【历史对话摘要】：用户名叫张三。之前用户请求生成了一张猫咪图片
和一只狗狗图片，用户对生成结果比较满意。
消息7-12: (原文保留)
```

## 4.3 动手实现

### 4.3.1 核心函数和返回结构

新建一个文件，在其中定义一个聊天消息（也就是上下文）压缩函数，返回优化后的消息和摘要（摘要单独返回更灵活，由调用方决定如何合并到 System Prompt）。

```typescript
// 核心压缩函数
async function compressHistory(messages): Promise<CompressResult>
// 返回结果类型
interface CompressResult {
  messages: ModelMessage[];  // 优化后的消息（不含 system）
  summary: string | null;    // 历史摘要
}
```

### 4.3.2 压缩函数关键步骤

**接着实现压缩函数的第一步：分离消息类型。**

在向大模型发起的请求里，包含了内置提示词（消息类型为 system）、用户的消息（消息类型为 user）和 AI 之前返回的消息（消息类型为 assistant）。

我们要压缩的是 用户消息和 AI 返回的消息：

```typescript
// 只处理 user 和 assistant 的对话
const dialogueMessages = messages.filter(m => m.role !== 'system');
// 提取已有的摘要（如果之前压缩过）
const systemMsg = messages.find(m => m.role === 'system');
```

**第二步是：判断当前是否需要压缩：**

```typescript
if (dialogueMessages.length <= MAX_MESSAGES) {
  return { messages: dialogueMessages, summary: existingSummary };
}
```

当消息比较少时不压缩。

> 这里只判断了消息个数，简单处理了，实际工作里还需要判断消息的长度，比如有大图片数据的情况。

**第三步切分消息，分理处需要压缩的：**

```typescript
const messagesToKeep = MAX_MESSAGES - SUMMARY_THRESHOLD;  // 10 - 4 = 6
const toCompress = dialogueMessages.slice(0, length - messagesToKeep);
const recentMessages = dialogueMessages.slice(length - messagesToKeep);
```

这里的逻辑细节：

-   假设有 12 条消息
    
-   保留最近 6 条（索引 6-11）
    
-   压缩前面 6 条（索引 0-5）
    

最近的不压缩，主要是为了减少信息丢失。

**第四步：调用 AI 生成摘要**

```typescript
const { text: newSummary } = await generateText({
  model: summaryModel,
  prompt: `你是一个对话摘要助手...
之前的对话摘要：${existingSummary}
最近需要压缩的对话：${dialogueText}
请生成简洁的摘要，保留关键信息...`,
});
```

这里用了一个专门的"摘要模型"（可以用便宜的模型），让它：

1.  理解之前的摘要（累积压缩）
    
2.  理解新的对话内容
    
3.  生成新的综合摘要
    

生成摘要的提示词很关键，实际工作里会根据压缩结果持续优化。

**第五步也是最后一步，在核心的大模型调用前使用压缩函数处理上下文：**

```none
   // 应用短期记忆管理：滑动窗口 + 摘要压缩
    const { messages: optimizedMessages, summary } = await compressHistory(modelMessages);
    console.log(`[记忆管理] 原始消息数: ${modelMessages.length}, 优化后: ${optimizedMessages.length}, 有摘要: ${!!summary}`);
    // 基础 System Prompt
    const baseSystemPrompt = `你是一个多模态 AI 助手，可以帮助用户：
1. 进行文本对话
2. 生成手绘风格的知识图片
3. 生成视频
工具使用规则：
- generateImage: 当用户明确要求生成图片时使用（关键词：生成图片、画一张、帮我生成一张图、创建图片等）
- generateVideo: 当用户明确要求生成视频时使用（关键词：生成视频、做一个视频、帮我生成一个视频、创建视频等）
- 其他情况：使用普通文本回复
重要提示：
- 只有当用户明确表达生成图片或视频的意图时，才调用工具
- 如果用户只是询问"什么是图片生成"或"什么是视频生成"，不要调用工具，而是用文本解释
- 如果用户说"我想看看图片"或"我想看看视频"，不要调用工具，而是询问用户想看什么内容
- 从用户输入中提取 prompt 参数时，要准确理解用户的意图，不要遗漏关键信息`;
    // 构建包含历史摘要的完整 System Prompt
    const fullSystemPrompt = buildSystemPromptWithSummary(baseSystemPrompt, summary);
    // 使用 streamText 生成流式响应，并注册工具
    const result = await streamText({
      model: deepseek,
      messages: optimizedMessages,
      system: fullSystemPrompt,
      tools: {
        generateImage,
        generateVideo,
      },
    });
```

整体代码截图：

![](https://static.xiaobot.net/file/2026-01-14/21332/2bb19bb3fc170d29731b38fbb754b7cd.png!post)

> 代码链接：[https://github.com/shixinzhang/AI-TODO/blob/main/lib/memory-manager.ts](https://github.com/shixinzhang/AI-TODO/blob/main/lib/memory-manager.ts)

### 4.3.3 别忘了错误处理

最后，我们还需要考虑到摘要生成失败（网络问题、API 限流等）的情况，这种情况下不能让整个对话崩掉，一般会这样处理：

-   降级到简单截断
    
-   保留之前的摘要，不丢失历史信息
    

## 五、实战二：给聊天加上长期记忆

短期记忆解决了“当前对话”的记忆问题，但如果用户新开了一个会话，AI 就会变成“最熟悉的陌生人”。

要让 AI 真正具有“人的温度”，我们需要给它加上**长期记忆**。在 AI 工程中，最常见的长期记忆方案就是生成**用户画像**。

### 5.1 第一步：定义画像数据结构

我们要明确 AI 应该记住哪些信息。

这里和上一讲一样，使用 Zod 定义结构化数据，确保 AI 提取的结果是可控的。

```typescript
import { z } from 'zod';
// 定义用户画像的 6 个核心维度
export const UserProfileSchema = z.object({
  profession: z.string().optional().describe('用户的职业或身份'),
  technical_stack: z.array(z.string()).optional().describe('用户熟悉的技能或技术栈'),
  preferences: z.string().optional().describe('用户的工作习惯或偏好'),
  interests: z.array(z.string()).optional().describe('用户的兴趣爱好'),
  communication_style: z.string().optional().describe('用户喜欢的交流风格（如：专业、幽默、简洁）'),
  goals: z.string().optional().describe('用户当前的主要目标'),
});
export type UserProfile = z.infer<typeof UserProfileSchema>;
```

这样的 Schema 提供给大模型后，在请求的时候在 response\_format 字段中体现：

```none
🌐 [网络请求] 发送请求到大模型 API
📍 URL: https://sg.uiuiapi.com/v1/chat/completions
📦 Method: POST
📋 请求体: {
  "model": "deepseek-chat",
  "response_format": {
    "type": "json_schema",
    "json_schema": {
      "schema": {
        "$schema": "http://json-schema.org/draft-07/schema#",
        "type": "object",
        "properties": {
          "profession": {
            "description": "用户的职业或身份，如：程序员、设计师、学生",
            "type": "string"
          },
          "technical_stack": {
            "description": "用户提到的编程语言或技术栈，如：[\"Python\", \"React\", \"AI\"]",
            "type": "array",
            "items": {
              "type": "string"
            }
          },
          "preferences": {
            "description": "用户的显性偏好，如喜欢的风格、讨厌的东西",
            "type": "string"
          },
          "interests": {
            "description": "用户的兴趣领域，如：[\"科幻\", \"音乐\", \"游戏\"]",
            "type": "array",
            "items": {
              "type": "string"
            }
          },
          "communication_style": {
            "description": "用户偏好的沟通方式，如：简洁直接、详细解释、幽默风趣",
            "type": "string"
          },
          "goals": {
            "description": "用户当前的目标或正在做的项目",
            "type": "string"
          }
        },
        "additionalProperties": false
      },
      "strict": true,
      "name": "response"
    }
  }
```

response\_format 字段的作用就是告诉 AI 期望的返回格式。

### 5.2 第二步：实现提取逻辑

```typescript
// lib/memory-manager.ts
import { generateObject } from 'ai';
import { UserProfileSchema } from './schema';
export async function extractProfileFromMessage(
  userId: string,
  userMessage: string,
  existingProfile: UserProfile
) {
  try {
    const { object: newTraits } = await generateObject({
      model: deepseek, // 建议用性价比高的模型
      schema: UserProfileSchema,
      prompt: `
        你是个资深的用户画像专家。以下是用户刚才说的话和已有的画像。
        请分析用户的话，提取新的个人特征，并与旧画像合并。
        【用户消息】：${userMessage}
        【现有画像】：${JSON.stringify(existingProfile)}
      `,
    });
    // 将新特征合并到数据库
    await updateUserProfile(userId, newTraits);
  } catch (error) {
    console.error('[画像提取失败]', error);
  }
}
```

提取逻辑就是把用户每轮新输入的内容和历史的画像提供给大模型，让它做新特征生成。

### 5.3 第三步：在对话接口中集成

有了提取逻辑后，接下来就是每次用户调用模型时使用它，具体有四步：

1.  读取之前的长期记忆
    
2.  将画像注入 System Prompt，让 AI 知道用户是谁
    
3.  对话结束后生成新画像
    

对应的代码：

```typescript
// api/chat/route.ts
export async function POST(req: Request) {
  const { messages, userId = 'default_user' } = await req.json();
  // 1. 读取长期记忆（画像）
  const userProfile = await getUserProfile(userId);
  // 2. 应用短期记忆管理
  const { messages: optimizedMessages, summary } = await compressHistory(messages);
  // 3. 构建完整 Prompt（基础指令 + 画像 + 短期摘要）
  const systemPrompt = `
    你是一个全能助手。
    【用户画像】：${JSON.stringify(userProfile)}
    【历史摘要】：${summary || '暂无'}
    请结合上述信息提供个性化回复。
  `;
  // 4. 调用流式模型，并注册旁路提取
  return streamText({
    model: deepseek,
    messages: optimizedMessages,
    system: systemPrompt,
    onFinish: async ({ text }) => {
      // 对话结束后，异步提取画像（不阻塞前端）
      const lastUserMsg = messages.filter(m => m.role === 'user').pop()?.content;
      if (lastUserMsg) {
        extractProfileFromMessage(userId, lastUserMsg, userProfile);
      }
    },
  });
}
```

streamText 函数的 onFinish 回调会**在回复前端结束后触发**，我们可以在这里做画像提取。

### 5.4 画像合并策略

当提取到新信息后，合并逻辑需要兼容不同类型的信息：

-   **字符串字段**：只需要一份，新信息覆盖旧信息（如：职业从“学生”变更为“前端工程师”）。
    
-   **数组字段**：去重合并（比如兴趣爱好字段，要不断累加）。
    

```none
export async function updateUserProfile(userId: string, newTraits: UserProfile): Promise<void> {
  try {
    // 读取现有画像
    const existingProfile = dbGetProfile(userId) || {};
    // 智能合并画像（新特征覆盖旧特征，数组去重合并）
    const mergedProfile: UserProfile = {
      profession: newTraits.profession || existingProfile.profession,
      technical_stack: mergeArrays(existingProfile.technical_stack, newTraits.technical_stack),
      preferences: newTraits.preferences || existingProfile.preferences,
      interests: mergeArrays(existingProfile.interests, newTraits.interests),
      communication_style: newTraits.communication_style || existingProfile.communication_style,
      goals: newTraits.goals || existingProfile.goals,
    };
    // 清理空值
    const cleanedProfile = Object.fromEntries(
      Object.entries(mergedProfile).filter(([_, v]) => v !== undefined && v !== null && v !== '')
    ) as UserProfile;
    // 保存到内存数据库
    dbSaveProfile(userId, cleanedProfile);
    console.log(`[画像管理] 💾 成功更新用户 ${userId} 的画像:`, cleanedProfile);
  } catch (error) {
    console.error('[画像管理] 更新用户画像异常:', error);
  }
}
```

合并完后，再持久化到数据库中（这里为了方便演示就保存到了内存中，实际生成项目要写到数据库）。

### 5.5 结合案例理解流程

为了更直观地理解理解，我们来看一个具体的案例日志。

我先和 AI 聊了一段时间，AI 已经记住了我是“全栈工程师”。

![](https://static.xiaobot.net/file/2026-01-14/21332/70da9c351e7875aa263889ab5b6d2195.jpeg!post)

现在我开启了一个新会话，问道：“你知道我什么信息？”，具体流程是这样。

**1\. 之前的画像注入到系统提示词（长期记忆生效）**

在请求大模型 API 时，系统会自动注入之前存好的画像：

```javascript
{
  "role": "system",
  "content": "...【用户画像】：职业：全栈工程师；技术栈：AI；兴趣：AI、人工智能\n\n【画像使用指引】\n- 用户画像是已知信息，你可以自由使用这些信息来个性化回答\n- 当用户问\"你知道我什么信息\"时，要基于画像回答..."
}
```

2\. AI 的深情回复

因为有了画像，AI 不再回答“我不认识你”，而是说：

> “根据你的用户画像，我知道你的职业是全栈工程师，技术栈涉及 AI 领域，同时你的兴趣也在 AI、人工智能方面~”

3\. 更新记忆

在回复完成后，onFinish 触发了一个隐藏的分析任务：

```javascript
// AI 内部的分析指令
{
  "role": "user",
  "content": "你是一个用户画像分析专家。请从对话中提取用户的特征信息。\n\n【对话内容】\n用户说: \"你知道我什么信息\"\nAI 回复: \"根据你的用户画像，我知道你的职业是全栈工程师...\"\n\n【现有画像】\n职业：全栈工程师；技术栈：AI；兴趣：AI、人工智能..."
}
```

通过这种循环，AI 的记忆会像滚雪球一样，随着交流的深入变得越来越精准。

> 代码链接：[https://github.com/shixinzhang/AI-TODO/blob/main/lib/memory-manager.ts](https://github.com/shixinzhang/AI-TODO/blob/main/lib/memory-manager.ts)

## 六、结语：AI 应用必须要做到的--越聊越懂

上下文工程，是 AI 应用从“Demo”到“产品”的分水岭。用户不会在意你用了多先进的模型，但他们会在意：

-   AI 是否记得我的偏好
    
-   对话是否流畅不卡顿
    
-   需求是否需要重复说
    

**这些细节，就是上下文工程的价值。**

读完这一讲，你可以有这些收获：

-   理解 Atkinson-Shiffrin 记忆模型，明白了为什么要分层管理
    
-   掌握了转移、压缩、缓存三大提示词工程策略
    
-   手写了短期记忆模块，实现滑动窗口 + 摘要机制，解决了 Token 爆炸问题
    
-   也实现了长期记忆功能，实现了隐式画像提取，让 AI 越聊越懂用户
    

我们今天手动实现了长短期记忆，核心目的是理解核心策略和细节。

真正生产项目里一般会用成熟的框架实现，比如 LangChain（一个 Agent 开发框架，后面会讲）就提供了一些处理上下文工程（在 LangChain 里叫 Memory）的 API，包含：

-   **全量存储对话的** ConversationBufferMemory
    
-   **存最近k轮的** ConversationBufferWindowMemory
    
-   **压缩历史为摘要的** ConversationSummaryMemory
    
-   ...
    

这些 API 通过不同存储策略管理对话历史，适配短对话全量记忆、长对话压缩/精准召回等场景，后面学习到 Agent 部分我们会用到。

这些 API 的功能和我们手撸的很像是吧。正因为学了今天这一课，当你在 LangChain 里调用这些 API 时，你不再是“黑盒用户”，而是真正懂得背后机制的**工程师**。

好了，这篇文章到这里就结束了，感谢你的阅读，我们下一讲见。

> 实战部分完整代码见：[https://github.com/shixinzhang/AI-TODO/blob/main/lib/memory-manager.ts](https://github.com/shixinzhang/AI-TODO/blob/main/lib/memory-manager.ts)

> 本专栏已开启「合伙人计划」，读者可生成专属的邀请链接或邀请海报。
> 
> 有人通过你的邀请链接或海报付费订阅时，会返现支付金额的 25% 给你，比如支付 298 元 会返现 74.5 元，快去分享给你的好朋友们吧！

💡 有启发

![](https://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83eqVMd6RvGU6TOPL3qByEYlKNfBygUP47qaZl1ngNQhd0NKQUQb5EGuHQMiaZP8qGEHSA1hjkBkWibsw/132)

转型 AI 工程师：重塑你的能力栈与思维

56 读者， 14 内容

![](https://xiaobot.net/img/icon_arrow_right_light.svg)