大家好，我是拭心。

从这一讲开始，我们进入第三阶段：**RAG 开发实战** 。

RAG 是目前企业招聘中**含金量最高、需求最迫切**的技能，贴一段招聘要求给大家看看：

![](https://static.xiaobot.net/file/2026-01-23/21332/e68b6aca4f3ae1ce2739d7006190b143.png!post)

这一讲我们先来整体认识 RAG，后面几讲通过实战深入掌握。

## 一、什么是 RAG

你有没有遇到过，当你咨询 ChatGPT/豆包 等 AI 聊天软件一些没那么通用的问题时，它会说一些正确的废话。

比如你问他：“我们公司今年的报销政策是什么？”

![](https://static.xiaobot.net/file/2026-01-23/21332/ced819750ff21e73375bceb395267abe.png!post)

它可能会回答：“你们公司今年的具体报销政策，我无法直接给出答案，因为这属于公司的内部管理规定。但我可以为你梳理一个企业报销政策的通用框架，并告诉你如何快速找到你们公司的准确规定。”

这不是废话吗？

这就是大模型的核心局限：**它并不是什么都懂。**

它能回答出公开的信息，但无法知道你们公司刚更新的报销额度。因为**它的知识来自训练时的数据，而你的公司文档从来没有出现在它的训练集里。**

**更要命的是时效性问题。**

就算它在训练时见过类似的政策文档，那也是训练时的版本。而你们公司的政策每年都在变，大模型的知识却停留在了它的知识截止日期。

有人说：“那为什么不微调一个专属大模型呢？” 问题是，微调有这些缺点：

-   **成本高**：微调一次可能要几千甚至几万美元
    

-   **周期长**：数据准备、训练、验证，没有几周搞不定
    

-   **维护难**：政策一改，又得重新微调一次
    

为了几页 PDF 去微调一个大模型，ROI 太低。

**所以我们需要给大模型加一个“外挂硬盘”**——不改变大模型本身，只是在它回答问题时，先帮它翻翻书，找到相关资料，再让它基于这些资料生成答案。

![](https://static.xiaobot.net/file/2026-01-23/21332/f761e7d8d9cb832ec1bc24be8791b21d.png!post)

这就是 **RAG（Retrieval-Augmented Generation，检索增强生成）**。

如果说大模型是一个博学但不了解新事物的老教授，那么 **RAG 就是教授手里的最新参考书**。他不需要背下所有知识，只需要知道去哪一页找答案。

## 二、RAG 的整体流程

理解 RAG 最好的方式，就是把它类比成考试。

以前问 AI 是“闭卷考试”——它只能靠训练时“背”下来的知识回答，忘了就只能瞎编。

现在 RAG 是“开卷考试”——它可以翻书，找到相关内容后再作答。

RAG 这三个字母拆开来看，就是整个流程：

-   **R（Retrieval，检索）**：先翻书，去知识库里找相关资料
    

-   **A（Augmented，增强）**：把找到的资料塞给 AI，增强它的“知识量”
    

-   **G（Generation，生成）**：AI 看着资料，生成最终答案
    

它的完整流程分两个阶段：

1\. 索引（考试前的准备）

2\. 查询（考试时的作答）

**阶段一：索引阶段**

这是“考试前的准备工作”，把你的私有文档（公司政策、产品手册、客户记录）整理成 AI 能快速查阅的“参考书”。

![](https://static.xiaobot.net/file/2026-01-23/21332/33da65f7b237b2fbf020a39257e2f811.png!post)

主要有四步：

1.  **文档导入**：把 PDF、Word、Markdown 等各种格式的文档读进来
    

2.  **文档分块（Chunking）**：把长文档切成小段，比如每段 500-1000 字左右
    

3.  **向量化（Embedding）**：把每个小段转换成一串数字（向量），代表它的“语义坐标”
    

4.  **存入向量数据库**：把这些向量存进专门的数据库（如 Milvus），等待检索
    

**阶段二：查询阶段**

这是“考试时的作答流程”，用户问问题，系统找资料，大模型生成答案。

![](https://static.xiaobot.net/file/2026-01-23/21332/438e4c00eb05a1e263f7565b46c795e6.png!post)

主要有这几步：

1.  **问题向量化**：把用户的问题也转成向量
    

2.  **相似度检索**：在向量数据库里找到和问题最相关的几个文档（比如 3-5 个文档块）
    

3.  **组装 Prompt**：把找到的资料和问题一起塞进 Prompt
    

4.  **大模型生成**：大模型看着这些资料，生成最终答案
    

整个过程，就像学生在开卷考试中的操作：

看到题目（用户问题）→ 翻书找相关章节（检索）→ 看着书上的内容作答（生成）。

**一句话总结，RAG 就是把问题相关的文档喂给 AI，让 AI 知道更多信息，然后回答。**

知道了整体流程，接着我们来看一些细节。

## 三、文档清洗与分块（Chunking）

索引阶段的第一步，就是把文档“切块”。

![](https://static.xiaobot.net/file/2026-01-23/21332/9a9e89f7eb6d54ced7607a0e363f340b.png!post)

为什么要切块呢？

首先是由于**大模型上下文窗口（Context Window） 的限制。**

大模型的上下文窗口（Context Window）是有限的。虽然有些模型能支持 200K tokens（约 20 万字），但这不意味着你可以把整个知识库都塞进去，原因是：

-   **成本高**：按 token 计费，塞得越多花得越多
    

-   **效果差**：信息太多会稀释重点，大模型容易“迷失”在海量信息中
    

**切块的另一个原因是为了更精准。**

切得越准，检索越准。如果你把一整篇 5000 字的政策文档作为一个单位存进去，那用户问“报销额度”时，系统会把整篇文档都返回——里面可能只有一句话是相关的，其余 4900 字都是噪音。

**所以说，好的分块策略，是在“语义完整性”和“Token 成本”之间走钢丝。**

知道了为什么切，接着就是怎么切。

**切分策略一：固定字符数**

最简单粗暴的方式：每 500 字一刀切。

```tap
原文：公司报销政策规定，差旅费用报销额度为每天 800 元，包括住宿费和交通费...（共 2000 字）
```

切分后：

-   Chunk 1：公司报销政策规定，差旅费用报销额度为每天 800 元..（500字）
    
-   Chunk 2：...包括住宿费和交通费，餐费另计..（500字）
    
-   Chunk 3：...超出部分需提前申请审批..（500字）
    
-   Chunk 4：...特殊情况可酌情处理。（500字）
    

**这种方式的优缺点：**

-   **优点**：实现简单，速度快。
    

-   **缺点**：可能把一句话切成两半，破坏语义连贯性。
    

**切分策略二：重叠切分（Overlap Chunking）**

这种方式在固定切分的基础上，让每个 Chunk 之间有 50-100 字的重叠：

> 原文：...差旅费用报销额度为每天 800 元，包括住宿费和交通费。餐费另计，标准为...
> 
> 切分后：
> 
> -   Chunk 1：...差旅费用报销额度为每天 800 元，包括住宿费和交通费。（500字，最后100字会重复到 Chunk 2）
>     
> -   Chunk 2：...包括住宿费和交通费。餐费另计，标准为每天 200 元..（从上一块的最后100字开始，再往下500字）
>     

**为什么要重叠一部分呢？**

假设关键信息“报销额度 800 元”刚好在 Chunk 1 的结尾，而它的解释“包含住宿和交通”在 Chunk 2 的开头。不重叠的话，检索时可能只找到半截信息——就像把一句话切成两半，前半句说“报销额度是”，后半句说“800 元”，单看哪一半都不完整。

> 一个小细节：重叠比例通常设置为 10%-20%（如 500 字的 Chunk，重叠 50-100 字）。

**切分策略三：语义切分（Semantic Chunking）**

语义切分是指切分的时候不按字数，而是按段落、小标题、语义单元来切：

> 原文：
> 
> \# 第一章 报销政策
> 
> \## 1.1 差旅费用
> 
> 差旅费用报销额度为每天 800 元...
> 
> \## 1.2 餐费标准
> 
> 餐费标准为每天 200 元...
> 
> 切分后：
> 
> -   Chunk 1：# 第一章 报销政策 ## 1.1 差旅费用 差旅费用报销额度为每天 800 元...
>     
> -   Chunk 2：## 1.2 餐费标准 餐费标准为每天 200 元...
>     

**这种方式的优缺点：**

**优点**：保证了内容的完整性，每个 Chunk 都是一个完整的语义单元。

**缺点**：实现复杂，需要识别文档结构（标题、段落、列表等）。

**实际场景下，固定切分 + 重叠 就够用了。**语义切分适合文档结构非常规范的场景（如技术文档、法律条文）。

**经验值**：

-   **Chunk 大小**：500-1000 字（中文）
    

-   **重叠比例**：10%-20%
    

-   **检索时返回**：Top-3 到 Top-5 个 Chunks
    

文档切好了，接下来要解决的是：计算机怎么理解文字的“意义”，比如“小狗”和“小猫”都是动物？

## 四、向量与 Embedding

传统数据库只能做精确匹配——你搜“苹果”，它只会找到文档里写着“苹果”两个字的地方。但如果文档里写的是“iPhone”或“iOS 设备”，传统搜索就迷茫了。

而 RAG 的检索需要的是**语义理解**——用户问“报销额度”，系统要能找到文档里写着“可报销金额”、“费用上限”的段落。

这就是 **Embedding（嵌入，或叫向量化）** 的作用。

### 4.1 Embedding 原理：把文字变成“语义坐标”

这个转换过程，是整个 RAG 系统最核心的部分。

可以说，**Embedding 是高纬度的映射。它把人类嘴里模糊的'语言'，翻译成了计算机能理解的精准的'数学坐标'。在向量空间里，词语不再是字符的拼接，而是意义的距离。**

**举个例子**：

> 1.  输入："公司的报销额度是 500 元"
>     
> 2.  Embedding 模型处理
>     
> 
> 3.  输出：\[0.012， -0.234， 0.881， 0.445， -0.667， ...\] （向量，一串浮点数）
>     

转换后的这串数字 \[0.012， -0.234， 0.881， 0.445， -0.667， ...\]，是这句话在**多维语义空间里的坐标**，叫做向量。

Embedding 模型，就是用来把文字转换成一串数字（向量）的模型。

**理解向量**

理解什么是向量（Vector）非常关键，这里我们再重点介绍一下它。

![](https://static.xiaobot.net/file/2026-01-23/21332/aed8f58e7a42d37d109a1aabb7a57085.png!post)

如果我们直接把文字存进数据库，它只能做文字匹配，比如你搜“报销”，它能找到“报销”，但找不到“退费”（因为字不一样）。

为了让电脑“读懂”文字背后的意思、找到之间的相似性，我们需要把信息变成一串**代表其“含义”的数字**。向量就是这个作用。

在游戏《英雄联盟》里，英雄的属性会有物理攻击、法术强度、生命值、攻击距离等维度：

1.  **物理攻击 (AD)**：代表平A 疼不疼。
    

2.  **法术强度 (AP)**：代表法术技能痛不痛。
    

3.  **生命值 (HP)**：代表肉不肉。
    

4.  **攻击距离 (Range)**：代表手长手短。
    

![](https://static.xiaobot.net/file/2026-01-23/21332/44844a113906ad231966bbfe04f66ecc.webp!post)

现在，我们把英雄联盟中最著名的英雄之一 德玛西亚之力（盖伦）的属性写下来（假设满分是 100）：

-   物理攻击：**90** (很高)
    

-   法术强度：**0** (完全没有)
    

-   生命值：**95** (非常肉)
    

-   攻击距离：**10** (近战短手)
    

于是，盖伦在电脑里就变成了这样一串数字：

> **盖伦的向量 =** **\[90, 0, 95, 10\]** 

我们再来看看盖伦的妹妹“拉克丝”的属性：

-   物理攻击：**20** (刮痧)
    

-   法术强度：**95** (爆发高)
    

-   生命值：**30** (脆皮)
    

-   攻击距离：**90** (手超长)
    

那么，拉克丝的向量就是：

> **拉克丝的向量 =** **\[20, 95, 30, 90\]** 

假设你想让系统给你推荐一个“和盖伦属性差不多”**的英雄，系统怎么找？它只需要**计算英雄属性的距离。

系统拿**盖伦的向量** \[90, 0, 95, 10\] 去数据库里搜索：

1.  **对比拉克丝**  **\[20, 95, 30, 90\]** ：
    

-   第一位 90 vs 20，差太多了！
    

-   第二位 0 vs 95，完全相反！
    

-   **结论：** 距离太远，这俩不是一类英雄，不推荐。
    

2.  **对比“诺克萨斯之手”**  **\[92, 5, 90, 15\]** ：
    

-   物理攻击：90 和 92（很像）
    

-   法术强度：0 和 0（都没有）
    

-   生命值：95 和 90（都很肉）
    

-   攻击距离：10 和 15（都是短手）
    

-   **结论：** **他们的属性距离极近，**诺手和盖伦的属性是相似的！
    

这就是**向量**。它不是什么高深的物理概念，它就是**把英雄的各项特征，压缩成的一串数字（实际情况下每位数字会缩小到 -1～1 ）**。

### 4.2 向量的维度：数字越多，表达越精确

实际情况下，Embedding 模型输出的不是 4 维，而是几百到几千维：

![](https://static.xiaobot.net/file/2026-01-23/21332/26ffe1f72f4155727aed2598e9f4ac88.png!post)

**为什么维度越高越好？** 就像用 GPS 定位，二维只能确定经纬度，三维加上海拔就更精确。维度越高，就像给语义空间增加更多坐标轴——不仅能区分“苹果”是水果还是手机，还能区分是“红富士”还是“iPhone 14 Pro Max”。

但维度高也意味着：

-   **存储成本增加**：1024 维 vs 3072 维，存储空间差 3 倍
    

-   **计算成本增加**：检索时要计算向量距离，维度越高越慢
    

所以实际应用中要权衡：通用场景用 1024-1536 维就够了，对精度要求极高的场景（如法律、医疗）才上 3072 维。

### 4.3 数学原理：余弦相似度

那怎么判断两个向量“距离近不近”？最常用的方法是**余弦相似度（Cosine Similarity）**。

**公式可以简单理解为这样**：

```abnf
相似度 = cos(θ) = 两个向量的点积 / (向量 A 的长度 × 向量 B 的长度)
```

计算结果在 -1 到 1 之间：

-   **1**：完全相同（夹角 0 度）
    

-   **0**：完全无关（夹角 90 度）
    

-   **\-1**：完全相反（夹角 180 度）
    

**举一个例子**：

> 问题向量："报销额度是多少？" → \[0.8， 0.5， 0.9\]
> 
> 文档 A 向量："报销额度为 500 元" → \[0.75， 0.52， 0.88\]
> 
> 文档 B 向量："今天天气不错" → \[0.1， 0.3， 0.2\]
> 
> 计算余弦相似度：
> 
> -   问题 vs 文档 A：0.98（非常相似）
>     
> -   问题 vs 文档 B：0.15（几乎无关）
>     

这种输入下，系统会选择相似度更高的文档 A 返回。

**小结一下**：向量数据库存的不是字，是这串“语义坐标”。检索的时候，系统拿着问题的坐标，去找离它最近的几个文档坐标，就完成了“语义搜索”。

## 五、向量数据库

文档变成向量后，下一步就是找个地方存起来，我们会用到向量数据库(Vector Database)。

### 5.1 为什么不用 MySQL？

有人会问：“MySQL 不也能存数字吗？为什么不直接建个表，把向量存进去？”

问题在于：**传统数据库擅长'精确匹配'，但不擅长'语义匹配'。**

MySQL 的查询逻辑是：

```n1ql
SELECT * FROM documents WHERE id = 1;
```

这是精确匹配——你要什么，我给什么。

但向量检索的逻辑是：

```accesslog
找出向量空间里，离 [0.8， 0.5， 0.9] 最近的 5 个向量
```

这需要计算距离（余弦相似度），如果有 1 亿条记录，MySQL 需要把 1 亿个向量都算一遍——这意味着查询一次可能要几分钟甚至几小时，完全无法用于实时应用。

**在 AI 的世界里，'像不像'比'是不是'更重要。这就是 Milvus 这种向量数据库存在的意义——它计算的是数据的相似度。**

### 5.2 认识 Milvus

**Milvus（** [**https://milvus.io/docs/zh**](https://milvus.io/docs/zh) **）**是一个开源的向量数据库，GitHub Stars 超过 **40K+**，专门为向量检索优化。

它的核心能力有这些：

**1\. 千亿级向量支持**

Milvus 可以处理 **千亿级（100 billion）** 的向量数据。什么概念？假设你有 1000 万篇文档，每篇切成 100 个 Chunk，就是 10 亿条向量——Milvus 能轻松搞定，还能再扩大 100 倍。

**2\. 毫秒级查询速度**

在 100 万条向量、768 维的数据集上，Milvus 的查询吞吐量达到 **946 QPS**（每秒查询次数），响应时间在毫秒级。

对比传统全文搜索引擎 Elasticsearch，Milvus 的查询速度**快 4 倍**。

**3\. 极致的内存优化**

**RaBitQ** 是一种极致的压缩技术，把原本需要 32 位存储的浮点数压缩到 1 位（0 或 1），就像把一张高清照片压缩成黑白素描——虽然细节有损失，但主要轮廓还在。

通过这项技术，Milvus 可以在保持 94.9% 召回率的前提下：

-   **内存减少 72%**（原来需要 1GB，现在只要 280MB）
    

-   **查询速度提升 4 倍**
    

**4\. 成本控制**

通过智能分层存储（热数据在内存，冷数据在磁盘），Milvus 可以让存储成本**降低 50%**。

**5\. 企业级特性**

-   支持多租户场景下单集群 **10 万+ Collections**（可以理解为 10 万张表）
    

-   云原生架构，支持水平扩展
    

### 5.3 和其他向量数据库对比

![](https://static.xiaobot.net/file/2026-01-23/21332/3f8738b8e3f566e8d56f22d32d335d5c.png!post)

**个人的选型建议：**

-   **生产级 RAG 系统**：用 **Milvus**（功能全，性能强，企业级首选）
    

-   **快速验证想法**：用 **Chroma**（轻量级，Python 环境友好，部署简单）
    

-   **离线批量处理**：**FAISS**（极致性能，Facebook 出品，适合本地研究）
    

-   **已有 PostgreSQL 技术栈**：**PGVector**（插件形式，技术栈兼容）
    

对于本专栏要实现的企业级 RAG 系统，我们会选择 **Milvus**——因为它是向量数据库领域的事实标准，兼顾性能、功能和生态。

## 六、检索（Retrieval）与生成（Generation）

知识库建好了（文档切块、向量化、存入 Milvus），现在用户来提问了。系统是如何在几毫秒内找到答案的？

让我们模拟一次完整的“开卷考试”流程。

### 6.1 步骤一：问题的向量化

用户问：“公司的报销额度是多少？”

系统第一步要做的，是把这个问题也变成向量——用**同一个 Embedding 模型**处理：

> 输入："公司的报销额度是多少？"
> 
> Embedding 模型（如 BGE-M3）处理
> 
> 输出：\[0.801， 0.512， 0.893， ...\] （1024 维向量）

**为什么必须用同一个模型？** 因为不同模型训练出来的向量空间不一样。就像 GPS 坐标系，如果你用的是火星坐标，我用的是地球坐标，那咱俩永远对不上。

### 6.2 步骤二：相似度计算（Similarity Search）

拿着问题的向量  \[0.801， 0.512， 0.893， ...\] ，去 Milvus 里找离它最近的几个文档块。

**参考代码**：

> search\_params = {
> 
> "metric\_type": "COSINE"， # 使用余弦相似度
> 
> "params": {"nprobe": 10} # 搜索精度参数
> 
> }
> 
> results = collection.search(
> 
> data=\[question\_vector\]， # 问题向量
> 
> limit=5， # 返回 Top-5
> 
> param=search\_params
> 
> )

**一个返回结果例子**：

![](https://static.xiaobot.net/file/2026-01-23/21332/78199c24e80f32efe0c7008e5e7e4c74.png!post)

前 3 条的相似度都在 0.88 以上，这就是向量检索的精准之处——它不是简单地匹配关键词，而是理解了“报销额度”、“可报销金额”、“报销标准”这些表达背后的同一个语义。

另外还可以看到：

-   **前 3 条**都和“报销额度”强相关，相似度在 0.88 以上
    

-   **第 4 条**虽然也提到报销，但说的是餐费，相关性略低
    

-   **第 5 条**几乎无关，相似度只有 0.52
    

**实际应用中，我们会这样配置参数**：

-   设置**相似度阈值**（如 0.7），低于阈值的直接过滤
    

-   返回 **Top-K**（通常 K=3 到 5），避免信息过载
    

### 6.3 步骤三：组装 Prompt（Prompt Engineering）

找到相关文档后，系统要把它们和用户问题一起“喂”给大模型。这一步叫 **Prompt 组装**。

我们可以这样拼接 **Prompt**：

> 你是一个专业的企业政策助手。请根据以下参考资料回答用户的问题。
> 
> 【参考资料】
> 
> 1\. 差旅费用报销额度为每天 800 元，包括住宿费和交通费。
> 
> 2\. 报销标准：国内出差每日 800 元，国际出差每日 1500 元。
> 
> 3\. 员工可报销金额上限为单次 800 元，超出部分需审批。
> 
> 【用户问题】
> 
> 公司的报销额度是多少？
> 
> 【回答要求】
> 
> \- 基于参考资料回答，不要编造信息
> 
> \- 如果资料中没有明确答案，请说明"资料中未提及"
> 
> \- 回答要简洁准确

**为什么要这样组装？**

这样的 Prompt 比较完整，就像考试时的题目——不仅告诉你问题是什么，还告诉你有哪些约束和已知条件。没有这些信息，大模型可能会天马行空地发挥，甚至编造一个听起来很像但完全错误的答案。

当你需要写自己的 RAG 拼接提示词时，记住这四点：

-   **明确角色**：“你是一个专业的企业政策助手” → 让大模型进入对应的“人设”
    

-   **提供上下文**：“参考资料” → 给大模型“开卷”的资料
    

-   **约束行为**：“不要编造” → 防止大模型瞎编（幻觉）
    

-   **规范输出**：“简洁准确” → 控制回答的格式
    

### 6.4 步骤四：大模型生成答案

把组装好的 Prompt 发送给大模型（如 GPT-4o、Claude、通义千问），它会基于参考资料生成答案：

> 根据公司政策，差旅费用报销额度为每天 800 元，包括住宿费和交通费。
> 
> 国内出差的标准是每日 800 元，国际出差为每日 1500 元。
> 
> 如果单次报销金额超过 800 元，需要提前申请审批。

**如果没有用 RAG ，AI 的回答会是这样，对比一下**：

![](https://static.xiaobot.net/file/2026-01-23/21332/3aadbb0ea341433fca852b380ce00949.png!post)

可以看到，两者的**核心区别是**：RAG 让大模型从“通用知识”模式切换到了“专家模式”——它不是凭记忆回答，而是看着你公司的真实文档回答。

### 6.3 完整流程

![](https://static.xiaobot.net/file/2026-01-23/21332/4306c4b456ff05f43605fdbef869db7d.webp!post)

看到这里，RAG 的整个流程就清晰又明确了：

> 用户提问
> 
> ↓
> 
> 【问题向量化】→ \[0.801， 0.512， ...\]
> 
> ↓
> 
> 【Milvus 检索】→ Top-5 相关文档块
> 
> ↓
> 
> 【Prompt 组装】→ 资料 + 问题 + 规则
> 
> ↓
> 
> 【大模型生成】→ 最终答案
> 
> ↓
> 
> 返回用户

## 七、结语

好了，以上就是我们这讲的全部内容，RAG 不仅仅是搜索，它做到了非常有价值的语义理解。

回顾一下我们今天学到的核心概念：

**RAG 的本质和关键**：

-   **RAG 的本质**：一场 AI 的“开卷考试”，让大模型在回答时能够翻阅参考资料
    

-   **Chunking（分块）**：好的分块策略，是在语义完整性和 Token 成本之间走钢丝
    

-   **Embedding（向量化）**：把文字变成“语义坐标”，在多维空间里计算意义的距离
    

**RAG 的核心技术：**

-   **Milvus（向量数据库）**：千亿级向量、毫秒级检索，企业级 RAG 系统的标准选择
    

-   **检索与生成**：从问题向量化到相似度计算，再到 Prompt 组装，完整的数据流
    

**掌握了 RAG，你就掌握了业务智能化的有效工具，有了 RAG 的 AI 才是懂业务的专家。**

知道原理还不够，下一讲我们将结合代码，深入理解如何实现 RAG。

我们下一讲见！

> 本专栏已开启「合伙人计划」，读者可生成专属的邀请链接或邀请海报。
> 
> 有人通过你的邀请链接或海报付费订阅时，会返现支付金额的 25% 给你，比如支付 298 元 会返现 74.5 元，快去分享给你的好朋友们吧！

💡 有启发

![](https://static.xiaobot.net/user-avatar/2026-01-23/103255/34233765f9bcff3cd1c8b5a944e9b505.png)

转型 AI 工程师：重塑你的能力栈与思维

56 读者， 14 内容

![](https://xiaobot.net/img/icon_arrow_right_light.svg)