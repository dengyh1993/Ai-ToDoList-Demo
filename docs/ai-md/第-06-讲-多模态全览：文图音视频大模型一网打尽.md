大家好，我是拭心。

上一讲我们学会了[如何优雅地调用大模型 API](https://xiaobot.net/post/8200e18a-693b-4dbd-a414-0c16715a6fe4)，只提到了文本模型这一种模型。

现实生活里，用户不会满足于敲键盘——他们要拍照、要说话、要生成视频，因此热门的 AI 应用，基本上要支持文本、图片、音频、视频多种模态的输入。

这一讲我们来了解多模态大模型和一些优秀的细分领域模型，并且使用多模态模型做一个 AI 视频。

## 一、认识多模态大模型

## 1.1 什么是多模态大模型

![](https://static.xiaobot.net/file/2026-01-04/21332/34eed0e6ad77192f896bfeb50dabcd88.png!post)

多模态大模型是指**以大语言模型（LLM）为"大脑"，能够接收、推理和输出多模态信息的模型**。

这里的"多模态"包括：

-   文本
    
-   图像
    
-   音频
    
-   视频等
    

它与传统 AI 的区别是：

-   **传统 AI**：仅处理单一模态，图像识别模型只能看图，语言模型只能读文字
    
-   **多模态大模型**：可以像人一样同时处理多种信息，比如理解图文、视频内容的深层含义
    

![](https://static.xiaobot.net/file/2026-01-03/21332/268815c1629fe19e1dbd9bfc41ed2425.png!post)

> 代表性大模型的发展时间轴，后期越来越多的多模态模型出现
> 
> 图片来自论文 [https://arxiv.org/pdf/2306.13549v4](https://arxiv.org/pdf/2306.13549v4)

多模态大模型的出现，使得以前无法做到的事情如今能够实现，比如：

1.  基于图片编写网站代码：输入一张网页截图，直接生成对应的 HTML/CSS 代码
    
2.  不需要 OCR 的数学推理：直接理解图片中的数学公式，给出解题步骤
    
3.  理解梗图的深层含义：不仅识别图像内容，还能理解其中的文化隐喻
    

以前要实现这些功能，我们需要 OCR 模型、代码生成模型、图像理解模型等多个模型分别处理，现在一个多模态大模型就能搞定——这就是"多模态"带来的质变。

## 1.2 多模态大模型的基本原理

在读者群里看到有朋友说，被 CEO 问起“什么是大模型”不知道怎么回答：

![](https://static.xiaobot.net/file/2026-01-03/21332/c31148bc6e634d81da3463fc2a7902ef.png!post)

这一讲我们结合多模态大模型简单介绍一些基本原理，以便不时之需。

![](https://static.xiaobot.net/file/2026-01-03/21332/c6bfba86dc07995eeb2f554c21eb3cb2.png!post)

如上图所示，多模态大模型的核心架构包括三个模块：

1.  模态编码器
    
2.  模态接口
    
3.  大语言模型
    

接下来我们逐个了解这三个模块，做应用开发不需要深刻记住这些，但**有个基本认知更利于日常和别人沟通、面试体现水平和做一些复杂的业务。**

### 1.2.1 模态编码器

![](https://static.xiaobot.net/file/2026-01-04/21332/60a3a0625ba82d9aaf97afc2713ae5a0.png!post)

**模态编码器负责将图像、音频等非文本信息转换为模型能理解的特征表示。**

**编码器对比**：

![](https://static.xiaobot.net/file/2026-01-03/21332/a8c4042a50e2d19b0a34f1da4941b7bb.png!post)

> 表格来自论文 [https://arxiv.org/pdf/2306.13549v4](https://arxiv.org/pdf/2306.13549v4)

可以看到，编码器的参数规模（Paramater Size）从 304M 到 1844.9M，但分辨率（Resolution）都在 224 左右——**这说明目前的瓶颈不在编码器大小，而在如何处理高分辨率输入。**

![](https://static.xiaobot.net/file/2026-01-04/21332/25da6b95c75b936d4b9b2cd5baa34c69.png!post)

目前主流大模型使用上图这两种方式处理高分辨率：

1.  **直接缩放**：微调编码器参数，以支持直接缩放处理更高分辨率图像（如 448x448）
    
2.  **补丁分割法**：
    
    1.  将大图分割成多个小补丁
        
    2.  小补丁捕捉局部细节，低分辨率图捕捉全局特征
        
    3.  最后汇总
        

简单的理解：**模态编码器就像 AI 的"眼睛"，把图片等格式的数据转换成数字信号**。就像你看一张照片，大脑会提取出"有个人、穿红衣服、在笑"这些不同特征，编码器做的就是这个工作。

不同类型的输入转换为统一的数字信号后，下一步就轮到了模态接口出马。

### 1.2.2 模态接口

![](https://static.xiaobot.net/file/2026-01-03/21332/ce2a9a10775944ea8b8a115b7bc6f379.png!post)

模态接口的作用是**将不同模态的特征对齐到 LLM 能理解的语义空间**。

这句话什么意思呢，简单讲**"模态接口"就是一个超级翻译官**。**图像编码器**说"火星语"的（因为图片是一堆像素数字）。而**大语言模型**是说"地球语"的（它只懂文字逻辑）。

如果直接把图像编码器的结果输入给大语言模型，大语言模型无法处理。因此中间加了一个**"模态接口"模块，**它的工作就是：**把图像的"火星语"翻译成大脑能听懂的"地球语"。**

**主流的模型使用了三种不同翻译的方式：**

1.  Q-Former (Querying Transformer)
    
2.  MLP（Multi-Layer Perceptron）
    
3.  特征级融合
    

**① Q-Former 派：**

![](https://static.xiaobot.net/file/2026-01-03/21332/6b790f39cdcba578ecafef2ae70665ef.png!post)

Q-Former 即 Querying Transformer，它是一个基于 Transformer 架构、通过主动提问（Querying）来从图像中提取信息的模块。

这种方式简单的理解，就是用一组可学习的"问题"去视觉特征里提取 LLM 需要的信息，就像你拿着问卷去采访，只保留最关键的答案。

使用这种方式的代表模型有 BLIP-2、InstructBLIP、Qwen-VL 等。**这种方式的优势是能够压缩视觉令牌数量，减少计算量。**

**② MLP 派**

![](https://static.xiaobot.net/file/2026-01-03/21332/84e2d1f8080ac0b5cee009bae7678fed.png!post)

MLP（Multi-Layer Perceptron，多层感知机）最简单粗暴，就像你出国旅游，为了解决插头无法插进插座，买了个**电源转接头**。

这种方式不做复杂的压缩或筛选，它的内部只有简单的数学公式（线性变换），简单地把图像数据的"形状"（维度）捏一捏，改成大脑能处理的形状，然后直接塞进去。

使用这种方式的代表模型是 LLaVA 系列。**这种方式的优势：参数量少，效果好。**

> **LLaVA** 的全称是 **Large Language and Vision Assistant**（大语言与视觉助手）。它是目前开源界最著名、影响力最大的多模态大模型之一。它几乎是开源多模态模型的"祖师爷"，代码简单易懂，任何人都可以用几张显卡复现它的效果。LLaVA 证明了**不需要从头训练一个超级大模型**。它用一个现成的视觉编码器（CLIP）+ 一个现成的语言模型（Vicuna/LLaMA），中间加个简单的**MLP 连接层**，就能达到不错的效果。

**③ 特征级融合派**

![](https://static.xiaobot.net/file/2026-01-03/21332/0d68471df2a1c77a90639b59d6798ec6.png!post)

前两种方法都是讲上一层的输入翻译后传给大脑（大语言模型），而“特征级融合派”这种方法是直接**做手术**！

它把图像信号直接**插到大脑的神经元中间**（Transformer 层内部）。 当大脑在思考文字的时候，图像信号会作为"外挂"直接参与每一次神经脉冲的传递。

**这种方式的优点是：对输入数据理解最深刻，效果最好，但缺点是训练复杂，算力消耗高。**

以上就是目前主流的三种模态接口实现方式。之所以需要这么多种融合方式，是因为他们在"计算效率"和"表达能力"间做权衡——Q-Former 能压缩令牌数量，MLP 简单高效，而特征级融合表达能力更强但计算成本高。

总的来说，**模态接口就像"翻译官"，把图像的"语言"翻译成文字模型能理解的"语言"。**就像你看到一只猫，大脑会自动把视觉信号转换成"猫"这个概念，接口做的就是这个翻译工作。

### 1.2.3 大语言模型

在多模态模型中，大语言模型占据了绝大多数的参数量，是绝对的主导。以 **Qwen-VL** 为例，我们可以看一眼它的参数分布情况：

![](https://static.xiaobot.net/file/2026-01-03/21332/1b0f70d1d14164662b79911efc95eb2f.png!post)

如上图所示， Qwen-VL 中三个模块的参数分别为：

-   Q-Former 为 0.08B（<1%）
    
-   编码器：1.9B（19.8%）
    
-   LLM：7.7B（80.2%）
    

可以看到，**LLM 占据了绝大部分参数，这也是为什么称其为多模态大模型的"大脑"。**LLM 是多模态大模型的决策中枢，负责调用知识、进行推理并生成最终的回答。

经过模态接口处理后的图像特征，被转换成了 LLM 熟悉的向量，LLM 处理它们和处理文本一模一样。

![](https://static.xiaobot.net/file/2026-01-03/21332/6032727397bd44db96950bd07f8086e2.png!post)

**假如用户上传一张图片并输入文本："帮我分析这张图"，模态编码器和模态接口会把图片转换为一堆二进制数据，然后和提示词一起传递给 LLM。LLM 看到的其实是：\[图像特征\] + \[帮我分析这张图\]。它把图像当成了一段特殊的"前缀文字"来阅读。**

在多模态模型中，LLM 不再仅仅是一个"聊天机器人"，它承担了更复杂的职责：

-   **语义理解**：它接收翻译过来的图像特征（看起来像乱码的向量）和用户的文本问题。
    
-   **世界知识库**：图像只告诉模型"这是一个红色的圆形水果"，而 LLM 的知识库告诉它"这是苹果，富含维生素，牛顿曾经被它砸过"。**图像提供视觉信息，LLM 提供背景知识。**
    
-   **逻辑推理（Reasoning）**：这是 2025 年模型最看重的能力。比如给一张复杂的报表图，LLM 需要像数学家一样一步步分析数据，而不是瞎猜。
    

以上就是多模态模型和它的基本原理，第一次看可能有点费劲，因此我提供了一些图，有余力可以结合图多看几次。

有人可能会问，我平时又用不到，懂这些原理有什么用？个人觉得**最大的好处是可以在内部进行分享和面试时能让人对你评价更高，这两点很关键，有可能决定了你的升职加薪。**

就拿我自己来说，几年前我还在做业务的时候面试字节的一个基础架构岗，能够成功面上，有一个非常关键的因素：我学习热修复时看到一个系统底层加载原理（dex2oat），当时看了几篇文章才搞懂，没曾想面试的时候刚好被问到这个，于是凭着记忆把看到的知识点讲了一些，虽然说的磕磕巴巴，但当时知道这个的应该很少，给我加分不少。

OK，言归正传。接下来我们来看看目前头部的一些不同模态的模型，了解它们有助于后续开发 AI 应用时技术选型。

## 二、常用的文本模型

## 1.1 Google Gemini 3 Pro

个人体验下来，目前文本模型能力最强的当属 Google 的 Gemini 3 Pro，我们线上的好几个业务的核心部分用的就是它。

![](https://static.xiaobot.net/file/2025-12-31/21332/8f4ef0872b4dbd7c44139f45f9ca4827.png!post)

**Gemini 3 Pro 的优势在于：超长的上下文（100 万 token）处理、强大的多模态推理能力。**

没做过复杂 AI 业务的朋友可能不清楚"**100 万 token**"意味着什么，这么说吧，它意味着你在和 AI 的单次对话时，提示词里面可以放进去一本书那么多的文字，这大大提升了 AI 能处理的数据量和业务复杂度，非常强悍！

具体来说，在这些场景有非常大的作用：

1.  AI 编程的场景：如果上下文够大，AI 就可以对整个代码库做分析和修改，大大提升 AI 编程效率
    

2.  文档分析总结场景：能支持多文件分析总结，非常适合需要大量资料综合对比的业务
    
3.  内容创作改编场景：长文创作、改编要求角色、背景、情节的强一致性，上下文够长可以很好的解决这个问题
    

除了文本处理，Gemini 3 Pro 也支持图片、音频、视频理解，[根据 OpenRouter 的数据](https://openrouter.ai/google/gemini-3-pro-preview/api)，有这些头部 AI 软件都在使用 Gemini 3:

![](https://static.xiaobot.net/file/2025-12-30/21332/2023cbcd3953605c7517595e97a762ad.png!post)

Gemini 3 Pro 的缺点在于：价格有点贵，是 DeepSeek V3.2 的 X 倍、豆包 1.6 的 Y 倍。

![](https://static.xiaobot.net/file/2025-12-30/21332/10ecf4bbd1ed82adbcd4eb2baff245fd.png!post)

另外它是分段计费（OpenRouter 的价格）：

-   ≤200K Token 时价格是 $2/百万 Token
    
-   \>200K Token 时 $4/百万 Token
    

所以，如果成本有限，可以考虑把输入少于 200K、又非常重要的任务交给它。

我们的线上业务的核心节点主要用 Gemini 3 做内容创作和方案构思，它对中文的支持很好，输出的内容质量很高，强烈推荐。

要使用 Gemini 3 pro，国外业务可以[通过 OpenRouter 的接口](https://openrouter.ai/google/gemini-3-pro-preview)：

```none
import OpenAI from 'openai';
const client = new OpenAI({
  baseURL: 'https://openrouter.ai/api/v1',  //OpenRouter 的 baseURL
  apiKey: '<OPENROUTER_API_KEY>',
});
// First API call with reasoning
const apiResponse = await client.chat.completions.create({
  model: 'google/gemini-3-pro-preview',  //模型名
  messages: [
    {
      role: 'user' as const,
      content: "How many r's are in the word 'strawberry'?",
    },
  ],
  reasoning: { enabled: true } 
});
```

国内业务建议自己搭建代理，比如通过香港/新加坡服务器转发。本地测试可以通过我们前面提到的 [UiUiAPI 聚合平台（https://sg.uiuiapi.com/pricing）：](https://sg.uiuiapi.com/pricing)

![](https://static.xiaobot.net/file/2025-12-31/21332/8780d684f705a3e71e3bc128649187bd.png!post)

> 我没有正式在线上使用过 UiUiAPI 接口，因此建议你在线上使用时先压测一下。

## 1.2 Doubao-Seed-1.6

[Doubao-Seed-1.6 是字节跳动的多模态深度思考模型，](https://console.volcengine.com/ark/region:ark+cn-beijing/model/detail?Id=doubao-seed-1-6)它的最大优势在于性价比高---推理能力比还不错，价格是 Gemini 3 的五分之一都不到。

逻辑推理能力对比图：

![](https://static.xiaobot.net/file/2025-12-31/21332/adfb07e1e2d96c8e4bde9854578cfa33.png!post)

> 这里没有介绍 doubao-seed-1.8 是因为它还没有出多久，我们在线上使用时优先使用稳定可靠的版本。

Doubao-seed-1.6 的价格图：

![](https://static.xiaobot.net/file/2025-12-31/21332/310de0dcd9c905272849a1d2e9e04287.png!post)

Doubao-seed-1.6 的另外几个优点是：

1.  TPM（Tokens Per Minute，每分钟tokens数量）比同类模型要高，达到 5000k
    
2.  RPM（Requests Per Minute，每分钟请求数）30k，也就是说 QPS（每秒请求数） 500
    

对于大部分业务，这两个指标已经可以支撑比较大量的请求，因此适合并发比较大的 C 端场景。

而**它缺点也很明显：上下文只有 256k，是 Gemini 3 Pro 的四分之一，因此不适合输入内容太长的场景。**

经过我们的线上业务对比， doubao-seed-1.6 的中文理解能力仅次于 Gemini 3 Pro，适合业务规模比较大的场景。

要使用 doubao-seed-1.6，通过[火山方舟大模型平台：](https://console.volcengine.com/ark/region:ark+cn-beijing/model/detail?Id=doubao-seed-1-6)

![](https://static.xiaobot.net/file/2025-12-31/21332/d54cf0999d14fa13029f5195504504dd.png!post)

```javascript
// 调用火山方舟豆包模型
const client = new OpenAI({
apiKey: process.env.DOUBAO_API_KEY,
baseURL: "https://ark.cn-beijing.volces.com/api/v3"
});
const response = await client.chat.completions.create({
model: "doubao-seed-1.6-flash",
messages: [{ role: "user", content: "你好" }]
});
```

## 1.3 DeepSeek V3.2

最后一个要介绍的文本模型是 DeepSeek V3.2。

根据官方介绍，它的推理能力对标 GPT-5，仅略低于 Gemini-3.0-Pro，但实际使用时，发现它的中文理解和生成质量不如前面的两个，只是胜在价格：

![](https://static.xiaobot.net/file/2025-12-31/21332/b9db405b87e2c4feb0f52bcb236b376f.png!post)

输入价格和豆包 Seed 1.6 查不到，但输出价格要便宜好几倍。

因此如果预算有限，且不是中文逻辑生成场景（比如代码生成、逻辑推理、数学计算），可以使用它。

## 二、图片模型

介绍完使用最多的文本模型，接下来我们来看看大家最爱玩的图片模型。

图像模型的用途主要有两种：**理解图片和生成图片。**

## 2.1 Google 的 Nano Banana Pro

Nano Banana 相信大家都不陌生，但是我们这里要介绍的是它的 Pro，这两个区别还是蛮大的。

> Nano Banana 是 Gemini 2.5 Flash Image 的别名，而 Nano Banana Pro 是 Gemini 3 Pro Image 的别名，基于 Gemini 3 Pro。

![](https://static.xiaobot.net/file/2025-12-31/21332/f93047e3f7f3c31d44447f1b30f78d4e.png!post)

谷歌的技术还是强大，Nano Banana Pro 目前在图片生成领域绝对领先，**它的强大之处在于：**

1.  **具备强大的逻辑推理和世界知识**：能理解复杂指令背后的逻辑，生成准确的信息图、流程图，非常适合做 PPT
    
    ![](https://static.xiaobot.net/file/2025-12-31/21332/2853c06cf414ec92129f2c0833fc6ea8.png!post)
2.  **可以比较好的渲染中文**：一定程度解决了 AI 生成的图片里中文模糊错乱的问题，虽然有时候还是会有点模糊，但起码不会写一些奇怪的文字
    
    ![](https://static.xiaobot.net/file/2025-12-31/21332/96d3c2bec587b633b1212114a9356ce2.jpeg!post)
3.  **多图融合与角色一致性**：最多可接受 14 张参考图，并在生成的图像中保持最多5个角色或物体的外观、风格高度一致，非常适合系列内容创作
    
    ![](https://static.xiaobot.net/file/2025-12-31/21332/f2a661e7d5fa6a5696bd302a3ffedb0e.png!post)
4.  **工作室级编辑控制**：可以轻松实现专业后期操作，如调整光线（昼夜切换）、改变焦点（景深）、色彩分级、切换镜头角度等。
    
    ![](https://static.xiaobot.net/file/2025-12-31/21332/3c2dd292f463224f376b6288b94e7515.png!post)

它的价格与调用量、具体服务商有关，对比下来在 [KIE 上最便宜，2K 大概 0.5 元一张](https://kie.ai/pricing)：

![](https://static.xiaobot.net/file/2025-12-31/21332/bfde7105018bf795834374ae6838e9bf.png!post)

手动使用它的几个方式（基本都需要梯子和付费）：

1.  Gemini 官网（我目前使用最多的方式）：[https://gemini.google.com](https://gemini.google.com/)
    
    ![](https://static.xiaobot.net/file/2025-12-31/21332/8d026a6114d3ae27ffd22e80753b3676.png!post)
2.  Google AI Studio：[https://aistudio.google.com （需要关联一个 Google Cloud Project）](https://aistudio.google.com/)
    
    ![](https://static.xiaobot.net/file/2025-12-31/21332/c9bafdcbee466ad8968b20ffbb975e2f.png!post)
3.  NotebookLM: [https://notebooklm.google.com/](https://notebooklm.google.com/)
    
    ![](https://static.xiaobot.net/file/2025-12-31/21332/7bf9634e719414a98102eb8cc835000f.png!post)
4.  Lovart：[https://www.lovart.ai/zh/home](https://www.lovart.ai/zh/home)
    
    ![](https://static.xiaobot.net/file/2025-12-31/21332/ce81e41cf64260be3174b30d0ddba893.png!post)

API 调用它的几个方式：

1.  通过 OpenRouter 使用，model 名为 google/gemini-3-pro-image-preview
    

```none
import OpenAI from 'openai';
const client = new OpenAI({
  baseURL: 'https://openrouter.ai/api/v1',
  apiKey: '<OPENROUTER_API_KEY>',
});
const apiResponse = await client.chat.completions.create({
  model: 'google/gemini-3-pro-image-preview',
  messages: [
    {
      role: 'user' as const,
      content: 'Generate a beautiful sunset over mountains',
    },
  ],
  modalities: ['image', 'text']
});
const response = apiResponse.choices[0].message;
if (response.images) {
  response.images.forEach((image, index) => {
    const imageUrl = image.image_url.url; // Base64 data URL
    console.log(`Generated image ${index + 1}: ${imageUrl.substring(0, 50)}...`);
  });
}
```

2.  通过 [KIE API 平台](https://kie.ai/nano-banana-pro)调用，调用方式大家直接看文档（[https://docs.kie.ai/market/google/pro-image-to-image](https://docs.kie.ai/market/google/pro-image-to-image)），这里只要知道这个平台目前最便宜
    
    ![](https://static.xiaobot.net/file/2025-12-31/21332/5ed6eb3530a4d8dc0971bf622813c617.png!post)
3.  通过 [UiUi API](https://sg.uiuiapi.com/pricing) 平台搜索 nano banana pro，这个平台也会提供不同分辨率的 API
    

![](https://static.xiaobot.net/file/2025-12-31/21332/8752cdee2c8cf4e8623c0aabfa501e4f.png!post)

这篇文章里的知识点图都是用 nano banana 生成的，建议大家去某鱼买一个 Gemini 3 Pro，可以直接在 [http://gemini.google.com](http://gemini.google.com/) 使用 nano banana 。

## 2.2 字节 Doubao Seedream 4.x

Nano Banana Pro 虽香，但缺点是国内调用比较慢，价格换算成人民币也有点贵。

如果预算有限、使用量比较大或者对响应速度要求更高，可以使用 豆包的图片模型 doubao seedream 4.0 （或者 4.5，差异不大），它的价格是 Nano Banana Pro 最便宜时的一半：

![](https://static.xiaobot.net/file/2025-12-31/21332/fa4d119fd5329e76e24c073a95617e05.png!post)

Doubao-Seedream-4.x 目前使用最多的产品是字节的[即梦](https://jimeng.jianying.com/ai-tool/home)，个人评价它的生图能力已经是国内一流，在中文提示词理解、国风审美方面做的比较好。

![](https://static.xiaobot.net/file/2025-12-31/21332/733defce642b58933c23209e2623460d.png!post)

它擅长的点：

-   中文文字生成质的飞跃：支持公式、表格、化学结构等复杂排版
    
    ![](https://static.xiaobot.net/file/2025-12-31/21332/80e6a2d513a34181c28411b78ca4d6d6.png!post)
-   毛笔字等复杂文字的渲染效果显著提升
    
    ![](https://static.xiaobot.net/file/2025-12-31/21332/e1c0657bf301bfb1c2e0673778469fb0.png!post)
-   国风和二次元风格
    
    ![](https://static.xiaobot.net/file/2025-12-31/21332/2cc1d89980aece6f1fa58d7f53d38630.png!post)

手动使用去即梦官网：[https://jimeng.jianying.com/ai-tool/home](https://jimeng.jianying.com/ai-tool/home)

API 调用通过火山引擎：[https://console.volcengine.com/ark/region:ark+cn-beijing/model/detail?Id=doubao-seedream-4-5](https://console.volcengine.com/ark/region:ark+cn-beijing/model/detail?Id=doubao-seedream-4-5)

## 三、音频模型

音频模型的使用场景包括**语音识别（STT）、文本转语音（TTS）和声音克隆。**

今年我在公司做了一个 AI 生成新闻专辑的功能，在选择音频模型时对比了好几家，最后的结论是 **MiniMax 的音频模型效果最好、最真实。**

![](https://static.xiaobot.net/file/2025-12-31/21332/666584476e05dd8b31cafff43f495ed2.png!post)

我们在抖音等平台刷到的**第一人称电影解说**，很多都是通过 Minimax 做的，比如这种：

![](https://static.xiaobot.net/file/2026-01-04/21332/d964b9d8061a64e25a2e8b3d81c676eb.png!post)

[https://www.douyin.com/jingxuan/search/%E7%AC%AC%E4%B8%80%E4%BA%BA%E7%A7%B0%E8%A7%A3%E8%AF%B4%E7%94%B5%E5%BD%B1?aid=1991b984-1644-4d4f-82f9-c7901415813d&modal\_id=7487559102273506594&type=general](https://www.douyin.com/jingxuan/search/%E7%AC%AC%E4%B8%80%E4%BA%BA%E7%A7%B0%E8%A7%A3%E8%AF%B4%E7%94%B5%E5%BD%B1?aid=1991b984-1644-4d4f-82f9-c7901415813d&modal_id=7487559102273506594&type=general)

Minimax 是 2022 年创立，他们的创始人闫俊杰是一个非常优秀的人，看过几次他的分享，理想主义+技术兼具，值的佩服（前段时间[罗永浩也和他访谈过](https://www.bilibili.com/video/BV11NmtBzE36/)，感兴趣可以看看）。有几个朋友在那边工作，天天加班，但好在公司发展很快，三年内在音频和视频模型方面都做到了比较不错的程度，预计 2026.1.9 在港股上市，替他们高兴。

![](https://static.xiaobot.net/file/2025-12-31/21332/0a789af3b273701f1f9e8e5c98e046f6.png!post)

Minimax 目前最新的音频模型是 speech-2.6-hd，它的核心优势：

-   市面最拟人化的音色：情感细腻，支持快乐、兴奋、悲伤、愤怒等广泛情感
    
-   支持极速克隆：一句话即可模仿用户音色
    

**适用场景：**

-   语音助手、AI 客服
    
-   有声书、播客生成
    
-   第一人称解说或者配音
    

手动使用方式：

-   国内版官网 [https://www.minimaxi.com/audio](https://www.minimaxi.com/audio)
    
-   国外版：[https://www.minimax.io/audio](https://www.minimax.io/audio)
    

![](https://static.xiaobot.net/file/2025-12-31/21332/e33a0c70b6709a39004a27f3beb95c79.png!post)

大家可以去体验一下他们的 TTS（文本转语音），提供的音色都非常逼近真人。另外音色克隆功能也很好用，后面的实战部分我们将会做一个音色克隆的例子。

API 调用可以看官方文档：[https://platform.minimaxi.com/docs/api-reference/speech-t2a-http](https://platform.minimaxi.com/docs/api-reference/speech-t2a-http)

## 四、视频模型

视频生成是多模态能力的"天花板"，也是工程难度最高的部分。我们重点来看三个：Veo3、可灵 2.6 和 Seedance 1.5

![](https://static.xiaobot.net/file/2025-12-31/21332/31700dbad954f557364255497068e2b4.png!post)

## 4.1 Google Veo3

[Veo3](https://deepmind.google/models/veo/) 是目前在**视频质量、叙事控制力和运动表现**上最顶级的模型之一（最新版本 3.1），尤其适合对镜头语言有精准要求的专业级创作。

它的优势在于：

1.  生成视频的同时，也会内置音效，浑然一体
    
2.  真实世界物理和音频技术带来了更高的真实感和保真度
    

3.1 版本的新特性：

1.  强一致性：生成的视频里会符合输入的角色图片
    
    ![](https://static.xiaobot.net/file/2025-12-31/21332/86d16faff3994890aa217f11ea3a55c3.png!post)
2.  镜头控制能力更强：可以控制镜头的位置、方向
    
    ![](https://static.xiaobot.net/file/2025-12-31/21332/f8ca19f79b2ba688c8ae21e4a18d02fb.png!post)
3.  支持拓展视频：输入一段视频，生成后面的情节，延用一致的视觉和听觉
    
    ![](https://static.xiaobot.net/file/2025-12-31/21332/10337939eaa9942b0834ce302648dd9b.png!post)
4.  首尾帧：输入第一帧和最后一帧，可以生成流畅、精美、震撼的过渡视频
    
    ![](https://static.xiaobot.net/file/2025-12-31/21332/98d5a3fa3657f4fd10e0c6a5668b0daa.png!post)
5.  角色控制：输入你的动作或者表情，让指定的角色复制你的行为
    
    ![](https://static.xiaobot.net/file/2025-12-31/21332/07cfaea8120afb7c38ac8475ced7917f.png!post)

Veo3 的手动使用方式：

1.  AI Studio
    
2.  Kie
    
3.  UiUi 聚合平台
    

API 调用方式：

[https://kie.ai/features/v3-api](https://kie.ai/features/v3-api)

![](https://static.xiaobot.net/file/2025-12-31/21332/ba5ed98c7046f1f8a0efb158b62c9664.png!post)

> [官方提供的 Veo 提示词指导：https://deepmind.google/models/veo/prompt-guide/](https://deepmind.google/models/veo/prompt-guide/)

## 4.2 快手 可灵

可灵是目前国内视频生成质量最高的模型之一，同名的官方平台地址为 [https://app.klingai.com](https://app.klingai.com/)

![](https://static.xiaobot.net/file/2026-01-04/21332/384a5e458d2112014130ca3e8cafada2.png!post)

可灵系列视频模型的核心优势是：

-   支持 24 fps 的 1080P 视频
    
    ![](https://static.xiaobot.net/file/2026-01-04/21332/ac2cc9e991e3f02c9bfd1c1de1378dd5.png!post)
-   支持多镜头叙事能力，适合剧情推进、分镜切换
    
-   新增音频能力：支持自动配音、上传自定义音频文件
    
-   API 开放度高，适合快速集成
    

25 年 12 月，可灵发布了最新的「视频 2.6 模型」，它可以在**一次生成中同时产出画面 + 自然语音 + 匹配音效 + 环境氛围**，实现了一键直出。无论是输入一段文字，还是上传一张图片，都能一键得到完整、有声、有节奏的动态视频，使用者不再需要费力拼接。

可灵模型的适用场景：

-   需要长视频生成（如广告、短剧）
    
-   多镜头叙事（如分镜脚本）
    
-   国内部署，API 开放度好
    
-   长视频生成、多镜头叙事
    

手动使用，前往可灵官网：[https://app.klingai.com/cn/](https://app.klingai.com/cn/)

API 调用方式可以参考官方文档：[https://app.klingai.com/cn/dev/document-api/quickStart/featureList](https://app.klingai.com/cn/dev/document-api/quickStart/featureList)

## 4.3 字节 Seedance

同样在 25 年 12 月，字节发布了视频模型 Seedance 的 1.5 pro 版本，这个版本也**做到了原生音画高精同步**，覆盖环境音、动作音、合成音、乐器音、背景音乐及人声等全场景。

![](https://static.xiaobot.net/file/2026-01-04/21332/5966e093d06b1707481ed8d3b81a1e4c.jpeg!post)

根据字节侧的测评，Seedance 1.5 pro 在文本生成视频（T2V）的对齐度（Alignment）指标上取得领先。

![](https://static.xiaobot.net/file/2026-01-04/21332/ef35a7fa271e3ca88a640719fc30dc1c.jpeg!post)

另外在音频能力评估中，Seedance 1.5 pro 在生成质量、同步性、对齐度、表现力等指标上超越 Veo 3.1 和 Kling 2.6。

要使用 Seedance 1.5 pro 最方便的方式是通过 即梦的视频生成：[https://jimeng.jianying.com/ai-tool/generate?type=video](https://jimeng.jianying.com/ai-tool/generate?type=video)

![](https://static.xiaobot.net/file/2026-01-04/21332/3ac1c1b00652c5586546d411f3386c20.png!post)

API 调用的话通过火山方舟：[https://console.volcengine.com/ark/region:ark+cn-beijing/model/detail?Id=doubao-seedance-1-5-pro](https://console.volcengine.com/ark/region:ark+cn-beijing/model/detail?Id=doubao-seedance-1-5-pro)

## 五、实战：手搓一个 AI 宠物视频

通过前面我们了解了常用的文本、图片、音频和视频模型，接下来**通过一个实践来感受一下他们的能力。**

最近 AI 宠物视频比较火，主要模式是让宠物模仿人类说话、打工，具体表现是有一个人类和它说话，宠物作出可爱或者可怜的回应。在抖音和 B 站上，这类视频获得了很大的热度。

我们来做一个这样的视频，最终效果类似这样：[https://v.douyin.com/TkADFd6Tsmg/](https://v.douyin.com/TkADFd6Tsmg/)

![](https://static.xiaobot.net/file/2026-01-04/21332/2b0cf2856e8d8baab2d5e127ce5d4f95.png!post)

拆解一下，要做出这样的视频，需要用到这些模型：

1.  文本模型：生成剧本和镜头动作
    
2.  图片模型：生成首帧图片
    
3.  音频模型：克隆音色
    
4.  视频模型：生成视频
    

## 5.1 生成对话素材

首先，我们需要确定对话内容，对话模式参考这个视频 [https://www.bilibili.com/video/BV14HjbzUEid](https://www.bilibili.com/video/BV14HjbzUEid)，主要有两个角色进行对话：

1.  妈妈说：长大后要不要养妈妈
    
2.  宝宝说：要
    
3.  妈妈说：没钱怎么办
    
4.  宝宝说：去找工作
    
5.  妈妈说：要是工作也找不到呢
    
6.  宝宝说：我去捡破烂养妈妈
    

妈妈的保留不动，我们把孩子的回答改成程序员口吻。让 AI 生成孩子的回答，提示词这样：

```
你是专业的反转、搞笑文案专家，将这段对话中的宝宝发言进行修改，以得到一段程序员喜欢看、和原文一样简短、有趣、让人有转发欲的对话（妈妈说的话原封不动），要求风格是大白话，简单易懂，给出 3 种不同的结果：妈妈说：长大后要不要养妈妈 宝宝说：要 妈妈说：没钱怎么办 宝宝说：去找工作 妈妈说：要是工作也找不到呢 宝宝说：我去捡破烂养妈妈
```

根据使用的模型不同，AI 可能会给出不同的回答，测试下来 豆包（[https://www.doubao.com/chat](https://www.doubao.com/chat)）返回的比较符合预期：

![](https://static.xiaobot.net/file/2026-01-04/21332/7ef87341dc690a8502c6b1e5f0aa7fcb.png!post)

我们选择其中的第二条并微调：

妈妈说：长大后要不要养妈妈

宝宝说：要

妈妈说：没钱怎么办

宝宝说：接外包搞钱

妈妈说：要是工作也找不到呢

宝宝说：去做自媒体带货养你

有了对话内容，接着还需要【动作描述】，这是为了让最终生成的视频里宠物除了说话还有些动作，提示词：

```
这是一段对话文本，为其中的宝宝生成动作描述和镜头语言（剪短一点），如：镜头推进，他咬了口包子，对着镜头哭着说。核心画面是可怜兮兮的边吃包子边哭的回答问题。【对话文本】：妈妈说：长大后要不要养妈妈 宝宝说：要妈妈说：没钱怎么办 宝宝说：接外包搞钱 妈妈说：要是工作也找不到呢 宝宝说：去做自媒体带货养你
```

AI 会提供类似这样的结果：

![](https://static.xiaobot.net/file/2026-01-04/21332/2f960dbdbfce8c6dcc8d0ec995058772.png!post)

## 5.2 生成图片素材

有了文稿后，接下来就是图片素材。

去百度搜索一张你喜欢的宠物图片，比如柴犬幼犬：

![](https://static.xiaobot.net/file/2026-01-04/21332/eb1b7b849bae82d6208b65e3dd68b636.png!post)

下载后，让 AI （这里我用[即梦 https://jimeng.jianying.com/ai-tool/generate](https://jimeng.jianying.com/ai-tool/generate)）把图片改得可爱一点，比如这样的提示词：

```
基于提供的图片，生成一张站立的哭泣小狗，手里拿着包子，眼角流着泪珠，真实风格
```

![](https://static.xiaobot.net/file/2026-01-04/21332/94354cd2b70209c787ae30f7d2e8472d.png!post)

生成了上图所示的图片，的确更可爱一些了哈。

## 5.3 生成声音素材

接着生成声音素材，声音素材包括两部分：

1.  妈妈说的话
    
2.  宝宝说的话
    

妈妈说的话从原视频里剪辑，下载 B 站视频有很多工具，这里我们通过 [https://peanutdl.com/zh/bilibili](https://peanutdl.com/zh/bilibili) 进行下载，复制粘贴视频链接即可：

![](https://static.xiaobot.net/file/2026-01-04/21332/672a6ffd0ed56c9616eb164482384f2a.png!post)

视频解析后，右键点击播放区域，选择 Save Video As 或者保存视频即可：

![](https://static.xiaobot.net/file/2026-01-04/21332/0adf7c04246cea62177f4f4dcfe9cb03.png!post)

下载视频后，用剪映打开它，剪出需要的妈妈声音：

![](https://static.xiaobot.net/file/2026-01-04/21332/c8c6ce8188e40077261f2f0572d33554.png!post)![](https://static.xiaobot.net/file/2026-01-04/21332/64f5cc7885175b85af404c7dbb0aa856.png!post)

导出三个音频文件，用于后续插入到视频中（注意时仅选择【音频导出】）。

![](https://static.xiaobot.net/file/2026-01-04/21332/b2c157652d06b1cfeba883a4766d89cd.png!post)

宝宝说的话我们用 AI 生成，主要有这三段：

-   要
    
-   接外包搞钱
    
-   去做自媒体带货养你
    

生成音频分两步：

1.  克隆音色：找一个好听的孩子声音，提取音色
    
2.  文生音频：用刚才提取的音色生成音频
    

我们克隆这个视频里的孩子声音，声音比较可爱：[https://www.bilibili.com/video/BV1R94y1G7Z8](https://www.bilibili.com/video/BV1R94y1G7Z8)

还是前面的方式下载视频、导入剪映、剪出小孩的声音：

![](https://static.xiaobot.net/file/2026-01-04/21332/d8146fbb2a155c2cf31b0e7c8400d017.png!post)

有了要克隆的孩子声音后，下一步就是克隆这个音色，用于后续生成视频。

克隆音色和生成音频，我们使用前面介绍的 Minimax 国外官网（国内官网没有音色克隆这个功能）： [](https://www.minimaxi.com/audio)[https://www.minimax.io/audio/voices-cloning](https://www.minimax.io/audio/voices-cloning)

![](https://static.xiaobot.net/file/2026-01-04/21332/90a44ae095c2b8ab2eb1d8a2a340933b.png!post)

打开网站后，上传音频，然后输入要预览的声音和预览文本，预览文本可以随便填：

![](https://static.xiaobot.net/file/2026-01-04/21332/513023d8711d726f8ed5cc0754b83923.png!post)

克隆完成后可以试听效果，如果杂音比较多或者音色不纯，可能是因为输入的声音除了小孩的还有其他声音，需要重新剪辑。

接下来就是到文生音频部分，输入要生成的文本，选择刚刚生成的音色，生成音频：

![](https://static.xiaobot.net/file/2026-01-04/21332/7fdc93360bd9de7e02f0e842dfdff859.png!post)

这里我们输入“<u>要&lt;#0.5#&gt;接私活搞钱&lt;#0.5#&gt;扒开源模板卖钱养你</u>”，中间的 <#0.5#> 表示要停顿 0.5s，方便后续剪出多条声音。

生成后保存，用于下一步生成视频。我生成的音频是这样：

[http://aod.cos.tx.xmcdn.com/storages/e02a-audiofreehighqps/69/75/GAqhfD0NL8p5AAHWNARS0u1c.mp3](http://aod.cos.tx.xmcdn.com/storages/e02a-audiofreehighqps/69/75/GAqhfD0NL8p5AAHWNARS0u1c.mp3)

## 5.4 生成视频素材

准备好文本和音频，接下来到了我们的最后一步：生成视频素材。

打开即梦：[https://jimeng.jianying.com/ai-tool/home?type=digitalHuman](https://jimeng.jianying.com/ai-tool/home?type=digitalHuman)，因为我们期望最终的视频里口型和内容尽量贴合，所以选择【数字人】：

![](https://static.xiaobot.net/file/2026-01-04/21332/5423598962d4b0d027231d471f9953af.png!post)

选择后，上传前面生成的狗子拿着包子的图片，然后点击旁边的创建音色：

![](https://static.xiaobot.net/file/2026-01-04/21332/55b3193e4d7e06ccdd8ee903ed6da48b.png!post)

选择我们刚才克隆好的音色（这里也可以直接使用原始声音，但原始声音会有杂音，使用克隆后的效果更好），等待创建音色，然后输入角色要说的话和动作描述：

![](https://static.xiaobot.net/file/2026-01-04/21332/f95b36d2a81aa052d94471f5c9c11a72.png!post)

接着点击左下角的【快速模式】，切换为【大师模式】，这个模式下生成的效果更好、时间更长：

![](https://static.xiaobot.net/file/2026-01-04/21332/1ac53c484aa06942aa010569dbaef12f.png!post)

提交任务后，等待一段时间，最终的视频效果就生成好了：

![](https://static.xiaobot.net/file/2026-01-04/21332/e6b99efe06ee3118dfb1c28068e7b66c.png!post)

这个视频只有狗子宝宝说的话，接下来做最后的剪辑工作：

1.  把生成的视频下载导入到剪映
    
2.  根据每句话的气口，拆分为三段
    
3.  在每段前加上（前面生成的）妈妈的话
    
4.  加上过渡画面
    
5.  导出为视频
    

![](https://static.xiaobot.net/file/2026-01-04/21332/37509158a4257c8b792fda48b2b0fefe.png!post)

最终视频效果见这里：[https://www.bilibili.com/video/BV15qijBfETE](https://www.bilibili.com/video/BV15qijBfETE)

> 最后这个剪辑能做多少算多少，核心是掌握前面的流程。
> 
> 相关素材上传到这里：[https://github.com/shixinzhang/AI-TODO/tree/main/assets/6\_ai\_video](https://github.com/shixinzhang/AI-TODO/tree/main/assets/6_ai_video)

我们用一张图片来总结一下这个 AI 宠物视频的制作过程：

![](https://static.xiaobot.net/file/2026-01-04/21332/65eed6cae1e851dde031e833ce5c4f06.png!post)

## 六、结语

到这里，这篇多模态大模型的文章就结束了，我们来回顾一下学到的知识点。

首先了解了多模态大模型的核心能力：

1.  多模态理解：同时处理图像、文本、音频等多种信息
    
2.  跨模态推理：基于多模态信息进行复杂推理
    

然后结合图文学习了多模态大模型的基本原理，主要包括三部分：

1.  模态编码器
    
2.  模态接口
    
3.  大模型
    

然后介绍了文本、图片、音频、视频的头部模型，知道了它们的特点和如何使用。如果你炒股，知道哪些新模型表现最好，也有助于你赚钱，比如快手发布可灵新模型就导致股价上涨。

![](https://static.xiaobot.net/file/2026-01-05/21332/64b56b4949093ea96ef8fa442caab17a.png!post)

最后通过做一个 AI 宠物视频使用了前面提到的一些模型/平台，加深了对它们的理解。**这个 流程是现在很多爆款短视频的制作流程，大家掌握后可以多实践一下，不一定哪个就火了**。

对于 AI 工程师来说，**掌握多模态大模型意味着拓展了能力边界：知道单一文本以外的模型，能够处理更复杂的实际问题。除了开发聊天应用，还可以去做生图、音视频理解和生成等业务，大大增加了能力范围。**

通过这篇文章，你应该对**多模态大模型是什么、有哪些、怎么用**有了全面的理解。多模态技术正在快速发展，从医疗影像到智能助手，从文档理解到具身智能，应用场景越来越广泛。当 AI 机器人真的发展到能像人一样，多模态大模型一定功不可没！

好了，这篇文章到这里就结束了，感谢你的阅读，我们下一篇见。

> 由于平台不支持复制代码，文中提示词上传在这里：
> 
> [https://github.com/shixinzhang/AI-TODO/blob/main/prompts/%E7%AC%AC6%E8%AE%B2.md](https://github.com/shixinzhang/AI-TODO/blob/main/prompts/%E7%AC%AC6%E8%AE%B2.md)

> 本专栏已开启「合伙人计划」，读者可生成专属的邀请链接或邀请海报。
> 
> 有人通过你的邀请链接或海报付费订阅时，会返现支付金额的 25% 给你，比如支付 298 元 会返现 74.5 元，快去分享给你的好朋友们吧！

💡 有启发

![](https://thirdwx.qlogo.cn/mmopen/vi_32/HuRNBbcBClXO0ywWdRKHctFnKA0TiaL5qqSQnC8Igwf1dib0RAd1YvVjwSwMwr8Zibwm7lVcjZMg7FpmC9VwO8UzVtzUuuCCjeZROpg7kuZwL8/132)![](https://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83ep8jKdX2LkxL7W86lqfKeHEJoiawz427qXL0prIibmG0Gt8lDo8zHGqdenF0MR9hqHtytdNjic96ibViaw/132)![](https://thirdwx.qlogo.cn/mmopen/vi_32/1xJwuMWRltghibThBy3nyHsXYfbjAYTrVibwLYsI277iaypDB290TDBWZ3gJibxK0XM4umkIgFGkH3aeSVyOZtKJLA/132)![](https://thirdwx.qlogo.cn/mmopen/vi_32/Kseualmmic616WSibwdLhMibDesnU6QUvQ1pZlJ2ans4tTz7DibwtFg0t5cJ767jPiaS0ibqAM1mgEe63Ho3libiblmvqw/132)

转型 AI 工程师：重塑你的能力栈与思维

56 读者， 14 内容

![](https://xiaobot.net/img/icon_arrow_right_light.svg)